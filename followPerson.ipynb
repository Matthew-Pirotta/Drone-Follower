{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual run that I found the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "\n",
    "- cv2 to capture the image\n",
    "- Yolo to detect the person - include this in litreature review\n",
    "- https://docs.ultralytics.com/models/yolov8/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial things to use the drone - new features\n",
    "# Tello does not work with out model \n",
    "\n",
    "from djitellopy import tello\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "\n",
    "me = tello.Tello()\n",
    "# To connect \n",
    "me.connect()\n",
    "\n",
    "# # To start \n",
    "me.streamon()\n",
    "\n",
    "# # To takeoff\n",
    "# me.takeoff()\n",
    "\n",
    "# Open a window to display the video feed\n",
    "while True:\n",
    "    # Get the video frame\n",
    "    frame = me.get_frame_read().frame\n",
    "\n",
    "    # Resize the frame for better display (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "    # img = cv2.cvtColor(framw, cv2.COLOR_BGR2RGB)   \n",
    "\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Tello Camera\", frame)\n",
    "\n",
    "    \n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    me.send_rc_control(10, 0, 0, 5)\n",
    "\n",
    "    # me.land()\n",
    "\n",
    "    \n",
    "\n",
    "# # To control movement\n",
    "# # me.send_rc_control(left/right, forward/backward, up/down, yaw_velocity)\n",
    "\n",
    "# # To stop - for 5 seconds\n",
    "# me.sleep(60)\n",
    "\n",
    "# # To land\n",
    "# me.land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# These values need to be fine tuned\n",
    "K_x = 0.05 # Left/Right movement scale\n",
    "K_y = 0.1  # Forward/Backward movement scale\n",
    "K_z = 0.02 # Up/Down movement scale\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "# Read the drone's initial position\n",
    "drone_x, drone_y, drone_z = 0, 0, 2 \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # Define screen center\n",
    "    frame_center_x = frame.shape[1] // 2  # Middle of frame\n",
    "    frame_center_y = frame.shape[0] // 2 \n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Calculate the change in area\n",
    "            area_change = abs(current_area - last_area)\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                # Left/Right Movement (X-axis)\n",
    "                drone_x += K_x * (person_x - frame_center_x)\n",
    "                # Forward/Backward Movement (Y-axis)\n",
    "                drone_y += K_y * ((last_area / current_area) - 1)\n",
    "                # Up/Down Movement (Z-axis)\n",
    "                drone_z += K_z * (frame_center_y - person_y)\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "                print(f\"New Drone Position: X={drone_x:.2f}, Y={drone_y:.2f}, Z={drone_z:.2f}\")\n",
    "                # Left/Right Movement\n",
    "                # if person_x < frame_center_x - 50:\n",
    "                #     direction = \"left\"\n",
    "                #     print(\"Move Left\")\n",
    "                #     # Send command to drone: move left\n",
    "                # elif person_x > frame_center_x + 50:\n",
    "                #     direction = \"right\"  \n",
    "                #     print(\"Move Right\")\n",
    "                #     # Send command to drone: move right\n",
    "\n",
    "                # # Forward/Backward Movement\n",
    "                # if myPersonListArea[i] < 5000:  # Adjust based on detection area\n",
    "                #     direction = \"forward\"\n",
    "                #     print(\"Move Forward\")\n",
    "                #     # Send command to drone: move forward\n",
    "                # elif myPersonListArea[i] > 15000:\n",
    "                #     direction = \"backward\"\n",
    "                #     print(\"Move Backward\")\n",
    "                #     # Send command to drone: move backward\n",
    "\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS :\n",
    "- If there are multiple people it might change the person it is tracking (Maybe for now I will test it with one person)\n",
    "- It is difficult to find how much the drone should move and I am not sure whether I am doing it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations -- things to check \n",
    "- Latency: Ensure minimal delay between detecting the red spot and sending commands.\n",
    "- Safety: Test in a controlled environment to ensure predictable movements.\n",
    "- Camera Feed Access: If using the drone’s camera feed, ensure you can stream it to your processing device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following a Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize Deep SORT Tracker \n",
    "tracker = DeepSort(max_age=10, n_init=3)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame \n",
    "    results = model(frame)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls))\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  # Increased from 10 to maintain track through brief occlusions\n",
    "    n_init=5,    # Need 5 consecutive detections to confirm track\n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  # Stricter appearance matching\n",
    "    embedder_model_name=\"osnet_x1_0\",  # Better ReID model\n",
    "    embedder = \"mobilenet\",\n",
    "    half=True  # Use FP16 for faster inference\n",
    ")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    " \n",
    "    # Sharpen the image to make better performance\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))  # Add cropped image\n",
    "\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is very good but a random error came out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=5,    \n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder_model_name=\"osnet_x1_0\",  \n",
    "    embedder=\"mobilenet\",\n",
    "    half=True  \n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image for better detection\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.8:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width = x2 - x1  \n",
    "                height = y2 - y1\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from the tracked person\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Take first detected face\n",
    "\n",
    "                # Check if face is already in database\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    track.track_id = matched_id  # Assign existing ID\n",
    "                else:\n",
    "                    face_db[track_id] = face_encoding  # Save new face\n",
    "\n",
    "        # Draw tracking box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track.track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker with Default Embedder\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",  # Use default DeepSORT embedder\n",
    "    half=True,\n",
    "    embedder_gpu=True  # Enable GPU if available\n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "# {track_id: unique_id}\n",
    "person_id_map = {} \n",
    "# Counter for unique IDs \n",
    "next_person_id = 1 \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width, height = x2 - x1, y2 - y1  \n",
    "\n",
    "                # Extract cropped image for embedding\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue  # Avoid invalid crops\n",
    "\n",
    "                area = width * height \n",
    "\n",
    "                # Append detection with an empty embedding (DeepSORT will handle it)\n",
    "                # Default 128-dim zero vector\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker with valid detections\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb()) \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                # Take first detected face\n",
    "                face_encoding = face_encodings[0]  \n",
    "\n",
    "                # Match face to existing IDs\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    # Assign stored ID / the same ID as before\n",
    "                    person_id_map[track_id] = matched_id  \n",
    "                else:\n",
    "                    # New person, assign a unique ID\n",
    "                    face_db[next_person_id] = face_encoding  \n",
    "                    person_id_map[track_id] = next_person_id  \n",
    "                    next_person_id += 1    \n",
    "\n",
    "        # Get the unique ID for this person\n",
    "        unique_id = person_id_map.get(track_id, track_id)\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {unique_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)    \n",
    "    \n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "face_db = {}  # Stores face encodings {unique_id: face_encoding}\n",
    "person_id_map = {}  # Maps track_id → unique_id\n",
    "next_person_id = 1\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            conf = box.conf[0].item()\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                \n",
    "                # Extract person crop\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Will store the final ID\n",
    "\n",
    "        if face_locations:\n",
    "            # Adjust coordinates to global frame\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use the first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.4)\n",
    "                    if match[0]:  # Found a match\n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    next_person_id += 1\n",
    "\n",
    "                # Update the mapping to the current track_id\n",
    "                person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # If no face was detected, check if we already assigned an ID to this track\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id, None)\n",
    "\n",
    "        # If track_id is new and has no face, assign a temporary unique ID\n",
    "        if assigned_id is None:\n",
    "            assigned_id = next_person_id\n",
    "            person_id_map[track_id] = assigned_id\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code for tracking all the persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "# Stores face encodings {unique_id: face_encoding}\n",
    "face_db = {}  \n",
    "# Maps track_id → unique_id\n",
    "person_id_map = {}  \n",
    "# Keeps track of assigned person IDs\n",
    "used_person_ids = set()  \n",
    "next_person_id = 1\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame \n",
    "    results = model(sharpened)\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0])\n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()\n",
    "            # Class 0 = \"person\", confidence > 70%\n",
    "            if cls == 0 and conf > 0.7:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                #Calculate the width and the height of the bunding box\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                # Calculate area\n",
    "                area = width * height\n",
    "\n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "                # Extract person crop\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Will store the final ID\n",
    "\n",
    "        if face_locations:\n",
    "            # Adjust coordinates to global frame\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use the first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.55)\n",
    "                    if match[0]:  # Found a match\n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    while next_person_id in used_person_ids:  # Ensure unique ID assignment\n",
    "                        next_person_id += 1\n",
    "\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    used_person_ids.add(next_person_id)\n",
    "                    next_person_id += 1\n",
    "\n",
    "        # Ensure a stable mapping for DeepSORT track_id\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id)\n",
    "\n",
    "        if assigned_id is None:\n",
    "            # Assign a new unique person ID only if no previous ID exists\n",
    "            while next_person_id in used_person_ids:\n",
    "                next_person_id += 1\n",
    "\n",
    "            assigned_id = next_person_id\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Store the mapping between DeepSORT track_id and the stable person_id\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code for tracking only the persons with id 1 - using face recognition this is very good but when I face backwards it no longer detects me. But this still gives the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "# Stores face encodings {unique_id: face_encoding}\n",
    "face_db = {} \n",
    "# Maps track_id → unique_id \n",
    "person_id_map = {}  \n",
    "# Keeps track of assigned person IDs\n",
    "used_person_ids = set()  \n",
    "# Start with id 1\n",
    "next_person_id = 1\n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "# Movement sensitivity\n",
    "pixel_threshold = 100  \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    # Kernel to sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    # Use the kernel to filter the image\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference with the sharpened image\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0])  \n",
    "            # Get the confidence score\n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Bounding box\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                # Calculate the width and the height\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                # Crop the person\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "                # If the cropped image has a size of 0 than there must have been an error\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Default 128-dim zero vector\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker - so the tracker will only track the detected people\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    # Track only ID=1 -> variables to hold everything with id=1\n",
    "    person_x, person_y, current_area = None, None, 0\n",
    "\n",
    "    # For each tracked object\n",
    "    for track in tracked_objects:\n",
    "        # If there is no tracking continue\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        # Get track ID\n",
    "        track_id = track.track_id\n",
    "        # Get bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        # Get the centre\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        # Get the area of the bounding box\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        # Get face encodings -> recognise the face\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        # Final ID assignment\n",
    "        assigned_id = None  \n",
    "\n",
    "        # If there was face recognitions\n",
    "        if face_locations:\n",
    "            # Get adjusted bounding box coordinates for face recognition\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "            # Get face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            # If a face was detected and recognized\n",
    "            if face_encodings:\n",
    "                # Use first detected face\n",
    "                face_encoding = face_encodings[0]  \n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    # Compare face encodings\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.55)\n",
    "                    # If match, assign the person ID - take the first match\n",
    "                    if match[0]:  \n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    \n",
    "                    while next_person_id in used_person_ids:  \n",
    "                        next_person_id += 1\n",
    "\n",
    "\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    used_person_ids.add(next_person_id)\n",
    "                    next_person_id += 1\n",
    "\n",
    "        # Ensure stable ID mapping\n",
    "        if assigned_id is None:\n",
    "            # Get the person ID\n",
    "            assigned_id = person_id_map.get(track_id)\n",
    "            # While the next ID is used\n",
    "            while next_person_id in used_person_ids:\n",
    "                # Increment the ID -> we are trying to find a new ID to not have multiple people with the same ID\n",
    "                next_person_id += 1\n",
    "\n",
    "            # The assigned ID becomes ecqual to the next person ID\n",
    "            # Since this is the first free ID\n",
    "            assigned_id = next_person_id\n",
    "            # Add this to the set of used IDs\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Store mapping\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Only track and draw for ID = 1\n",
    "        if assigned_id == 1:\n",
    "            # Get the x and y  co-ordinates, and the area of the person\n",
    "            person_x, person_y, current_area = cx, cy, area\n",
    "\n",
    "            # Draw tracking box only for ID = 1\n",
    "            cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Process movement of ID=1 - if these are not None then a person with ID=1 is found\n",
    "    if person_x is not None and person_y is not None:\n",
    "        # If a person with ID=1 had already been found before\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Use the Eulidean distance\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # If the distance moved is greater than the pixel threshold\n",
    "            if distance_moved > pixel_threshold:\n",
    "                # Print the movement details\n",
    "                print(f'ID={assigned_id} -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update the position and the area\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # If a person had never been found before\n",
    "        else:\n",
    "            # Print the new area and new center\n",
    "            print(f'ID={assigned_id} -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "            # Assign the new position and the new area that has been found to be found again when the person moves\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model using face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID=1 -> Area: 466101, Center: (751, 439)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 21:06:45.282 python[26012:340336] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-20 21:06:45.282 python[26012:340336] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID=1 -> Area: 231039, Center: (957, 518)\n",
      "Person moved 220.63 pixels, updating movement.\n",
      "ID=1 -> Area: 292353, Center: (768, 494)\n",
      "Person moved 190.52 pixels, updating movement.\n",
      "ID=1 -> Area: 292813, Center: (646, 493)\n",
      "Person moved 122.00 pixels, updating movement.\n",
      "ID=1 -> Area: 294372, Center: (524, 490)\n",
      "Person moved 122.04 pixels, updating movement.\n",
      "ID=1 -> Area: 308220, Center: (739, 481)\n",
      "Person moved 215.19 pixels, updating movement.\n",
      "ID=1 -> Area: 702900, Center: (927, 358)\n",
      "Person moved 224.66 pixels, updating movement.\n",
      "ID=1 -> Area: 316260, Center: (421, 397)\n",
      "Person moved 507.50 pixels, updating movement.\n",
      "ID=1 -> Area: 704601, Center: (964, 357)\n",
      "Person moved 544.47 pixels, updating movement.\n",
      "ID=1 -> Area: 316260, Center: (451, 397)\n",
      "Person moved 514.56 pixels, updating movement.\n",
      "ID=1 -> Area: 703610, Center: (983, 357)\n",
      "Person moved 533.50 pixels, updating movement.\n",
      "ID=1 -> Area: 317896, Center: (481, 397)\n",
      "Person moved 503.59 pixels, updating movement.\n",
      "ID=1 -> Area: 703610, Center: (1002, 357)\n",
      "Person moved 522.53 pixels, updating movement.\n",
      "ID=1 -> Area: 318528, Center: (512, 397)\n",
      "Person moved 491.63 pixels, updating movement.\n",
      "ID=1 -> Area: 702900, Center: (1021, 357)\n",
      "Person moved 510.57 pixels, updating movement.\n",
      "ID=1 -> Area: 319032, Center: (542, 396)\n",
      "Person moved 480.59 pixels, updating movement.\n",
      "ID=1 -> Area: 703610, Center: (1039, 357)\n",
      "Person moved 498.53 pixels, updating movement.\n",
      "ID=1 -> Area: 320170, Center: (572, 397)\n",
      "Person moved 468.71 pixels, updating movement.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m person_crop \u001b[38;5;241m=\u001b[39m sharpened[y1:y2, x1:x2]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Get face encodings -> recognise the face\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(person_crop, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Final ID assignment\u001b[39;00m\n\u001b[1;32m    115\u001b[0m assigned_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/face_recognition/api.py:121\u001b[0m, in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face\u001b[38;5;241m.\u001b[39mrect), img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face), img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, model)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/face_recognition/api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m face_detector(img, number_of_times_to_upsample)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    # The ammount of time an ID survives\n",
    "    max_age=10,  \n",
    "    # The ammount of times the processer checks if it is the same ID\n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    # The confidence it has to say that they are the same person\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "# Stores face encodings {unique_id: face_encoding}\n",
    "face_db = {} \n",
    "# Maps track_id → unique_id \n",
    "person_id_map = {}  \n",
    "# Keeps track of assigned person IDs\n",
    "used_person_ids = set()  \n",
    "# Start with id 1\n",
    "next_person_id = 1\n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "# Movement sensitivity\n",
    "pixel_threshold = 100  \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    # Kernel to sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    # Use the kernel to filter the image\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference with the sharpened image\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0])  \n",
    "            # Get the confidence score\n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Bounding box\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                # Calculate the width and the height\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                # Crop the person\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "                # If the cropped image has a size of 0 than there must have been an error\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Default 128-dim zero vector\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker - so the tracker will only track the detected people\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    # Track only ID=1 -> variables to hold everything with id=1\n",
    "    person_x, person_y, current_area = None, None, 0\n",
    "\n",
    "    # For each tracked object\n",
    "    for track in tracked_objects:\n",
    "        # If there is no tracking continue\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        # Get track ID\n",
    "        track_id = track.track_id\n",
    "        # Get bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        # Get the centre\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        # Get the area of the bounding box\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        # Get face encodings -> recognise the face\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        # Final ID assignment\n",
    "        assigned_id = None  \n",
    "\n",
    "        # If there was face recognitions\n",
    "        if face_locations:\n",
    "            # Get adjusted bounding box coordinates for face recognition\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "            # Get face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            # If a face was detected and recognized\n",
    "            if face_encodings:\n",
    "                # Use first detected face\n",
    "                face_encoding = face_encodings[0]  \n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    # Compare face encodings\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.55)\n",
    "                    # If match, assign the person ID - take the first match\n",
    "                    if match[0]:  \n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    \n",
    "                    while next_person_id in used_person_ids:  \n",
    "                        next_person_id += 1\n",
    "\n",
    "\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    used_person_ids.add(next_person_id)\n",
    "                    next_person_id += 1\n",
    "\n",
    "        # Ensure stable ID mapping\n",
    "        if assigned_id is None:\n",
    "            # Get the person ID\n",
    "            assigned_id = person_id_map.get(track_id)\n",
    "            # While the next ID is used\n",
    "            while next_person_id in used_person_ids:\n",
    "                # Increment the ID -> we are trying to find a new ID to not have multiple people with the same ID\n",
    "                next_person_id += 1\n",
    "\n",
    "            # The assigned ID becomes ecqual to the next person ID\n",
    "            # Since this is the first free ID\n",
    "            assigned_id = next_person_id\n",
    "            # Add this to the set of used IDs\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Store mapping\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Only track and draw for ID = 1\n",
    "        if assigned_id == 1:\n",
    "            # Get the x and y  co-ordinates, and the area of the person\n",
    "            person_x, person_y, current_area = cx, cy, area\n",
    "\n",
    "            # Draw tracking box only for ID = 1\n",
    "            cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "            if last_position is not None and last_area is not None and current_area > 0:\n",
    "                # Calculate distance moved in pixels\n",
    "                dx = person_x - last_position[0]\n",
    "                dy = person_y - last_position[1]\n",
    "                # Use the Eulidean distance\n",
    "                distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "                # If the distance moved is greater than the pixel threshold\n",
    "                if distance_moved > pixel_threshold:\n",
    "                    # Print the movement details\n",
    "                    print(f'ID={assigned_id} -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                    print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                    # Update the position and the area\n",
    "                    last_position = (person_x, person_y)\n",
    "                    last_area = current_area\n",
    "            # If a person had never been found before\n",
    "            else:\n",
    "                # Print the new area and new center\n",
    "                print(f'ID={assigned_id} -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                # Assign the new position and the new area that has been found to be found again when the person moves\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Recognition - this is saying that me and someone else are the same person which is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker with a Re-ID model\n",
    "tracker = DeepSort(\n",
    "    # Max-age is a hyper-parameter to say how much long an id is kept when it disappears \n",
    "    max_age=1000,  \n",
    "    # How much long it will take to shift track a new detection\n",
    "    n_init=5,    \n",
    "    max_iou_distance=0.5,\n",
    "    # To increase discrimination between people\n",
    "    # Slightly relax to prevent frequent ID switches\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    # Use GPU for embedding \n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "# Maps track_id → unique_id\n",
    "person_id_map = {}  \n",
    "# Keeps track of assigned person IDs\n",
    "used_person_ids = set()  \n",
    "# Start with ID = 1\n",
    "next_person_id = 1  \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "# Movement sensitivity\n",
    "pixel_threshold = 100  \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0])  \n",
    "            # Confidence score\n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Bounding box\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width, height = x2 - x1, y2 - y1\n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    # Track only ID=1\n",
    "    person_x, person_y, current_area = None, None, 0\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Assign a unique person ID based on DeepSORT's Re-ID features\n",
    "        assigned_id = person_id_map.get(track_id)\n",
    "\n",
    "        if assigned_id is None:\n",
    "            while next_person_id in used_person_ids:  \n",
    "                next_person_id += 1\n",
    "\n",
    "            assigned_id = next_person_id\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Only track and draw for ID = 1\n",
    "        if assigned_id == 1:\n",
    "            person_x, person_y, current_area = cx, cy, area\n",
    "\n",
    "            # Draw tracking box\n",
    "            cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Process movement of ID=1\n",
    "    if person_x is not None and person_y is not None:\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Body Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing all hyper-parameters to make better detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker with a Re-ID model\n",
    "tracker = DeepSort(\n",
    "    max_age=50, \n",
    "    # Require more frames before confirming a track \n",
    "    n_init=5,  \n",
    "    max_iou_distance=0.5,\n",
    "    # Slightly relax to prevent frequent ID switches\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Track the first detected person\n",
    "first_person_id = None  \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Track previous position for movement detection\n",
    "last_position = None\n",
    "last_area = None\n",
    "# Movement sensitivity\n",
    "pixel_threshold = 100  \n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image for better detection\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Bounding box\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                # Finding the width and the height \n",
    "                width, height = x2 - x1, y2 - y1\n",
    "\n",
    "                # Append the newly detected person\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    # Track only ID=1\n",
    "    person_x, person_y, current_area = None, None, 0\n",
    "\n",
    "    # Iterate over tracked objects to find ID=1 person\n",
    "    for track in tracked_objects:\n",
    "        # If the track was not confirmed\n",
    "        if not track.is_confirmed():\n",
    "            # Stop the for loop\n",
    "            continue\n",
    "\n",
    "        # Get the ID of the tracked object\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        # Find the centre of the object to mark is as the tracking point\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        # Find the area of the object to detect movement\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Assign first detected person as ID=1\n",
    "        if first_person_id is None:\n",
    "            # Lock in the first person detected\n",
    "            first_person_id = track_id  \n",
    "\n",
    "        # Only track first detected person\n",
    "        if track_id == first_person_id:\n",
    "            # Save the previously found results\n",
    "            person_x, person_y, current_area = cx, cy, area\n",
    "\n",
    "            # Draw tracking box\n",
    "            cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(sharpened, f\"ID: 1\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Process movement of ID=1\n",
    "    if person_x is not None and person_y is not None:\n",
    "        # If the person has already been identified before\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Find the change in the x-axis position\n",
    "            dx = person_x - last_position[0]\n",
    "            # Find the change in the y-axis position\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved in pixels\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # If the distance moved exceeds the threshold, print the details and update the last position and area.\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # If this is the first time tracking the person\n",
    "        else:\n",
    "            print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame \n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Body Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEMS ENCOUNTERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep sort causing different bounding area of the same person given different ids.\n",
    "- If a person goes missing and comes back you end up with a different ID.\n",
    "- If I cover the camera and uncover it I get an error but this should not be this case in our assignnment as the drone should never loose track of the person.\n",
    "- Tuning the tolerance of face detection was very difficult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
