{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual run that I found the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "\n",
    "- cv2 to capture the image\n",
    "- Yolo to detect the person - include this in litreature review\n",
    "- https://docs.ultralytics.com/models/yolov8/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] tello.py - 129 - Tello instance was initialized. Host: '192.168.10.1'. Port: '8889'.\n",
      "[INFO] tello.py - 438 - Send command: 'command'\n",
      "[INFO] tello.py - 462 - Response command: 'ok'\n",
      "[INFO] tello.py - 438 - Send command: 'streamon'\n",
      "[INFO] tello.py - 462 - Response streamon: 'ok'\n"
     ]
    },
    {
     "ename": "TelloException",
     "evalue": "Failed to grab video frames from video stream",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExitError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/djitellopy/tello.py:1049\u001b[0m, in \u001b[0;36mBackgroundFrameRead.__init__\u001b[0;34m(self, tello, address, with_queue, maxsize)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     Tello\u001b[38;5;241m.\u001b[39mLOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrying to grab video frames...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress, timeout\u001b[38;5;241m=\u001b[39m(Tello\u001b[38;5;241m.\u001b[39mFRAME_GRAB_TIMEOUT, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m av\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mExitError:\n",
      "File \u001b[0;32mav/container/core.pyx:374\u001b[0m, in \u001b[0;36mav.container.core.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/core.pyx:239\u001b[0m, in \u001b[0;36mav.container.core.Container.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/core.pyx:259\u001b[0m, in \u001b[0;36mav.container.core.Container.err_check\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/error.pyx:428\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mExitError\u001b[0m: [Errno 1414092869] Immediate exit requested: 'udp://@0.0.0.0:11111'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTelloException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # To takeoff\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# me.takeoff()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Open a window to display the video feed\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Get the video frame\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     frame \u001b[38;5;241m=\u001b[39m me\u001b[38;5;241m.\u001b[39mget_frame_read()\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Resize the frame for better display (optional)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/djitellopy/enforce_types.py:54\u001b[0m, in \u001b[0;36menforce_types.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     53\u001b[0m     check_types(spec, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/djitellopy/tello.py:421\u001b[0m, in \u001b[0;36mTello.get_frame_read\u001b[0;34m(self, with_queue, max_queue_len)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_frame_read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     address \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_udp_video_address()\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_frame_read \u001b[38;5;241m=\u001b[39m BackgroundFrameRead(\u001b[38;5;28mself\u001b[39m, address, with_queue, max_queue_len)\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_frame_read\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_frame_read\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/djitellopy/tello.py:1051\u001b[0m, in \u001b[0;36mBackgroundFrameRead.__init__\u001b[0;34m(self, tello, address, with_queue, maxsize)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress, timeout\u001b[38;5;241m=\u001b[39m(Tello\u001b[38;5;241m.\u001b[39mFRAME_GRAB_TIMEOUT, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m av\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mExitError:\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TelloException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to grab video frames from video stream\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker \u001b[38;5;241m=\u001b[39m Thread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_frame, args\u001b[38;5;241m=\u001b[39m(), daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTelloException\u001b[0m: Failed to grab video frames from video stream"
     ]
    }
   ],
   "source": [
    "# Initial things to use the drone - new features\n",
    "# Tello does not work with out model \n",
    "\n",
    "from djitellopy import tello\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "\n",
    "me = tello.Tello()\n",
    "# To connect \n",
    "me.connect()\n",
    "\n",
    "# # To start \n",
    "me.streamon()\n",
    "\n",
    "# # To takeoff\n",
    "# me.takeoff()\n",
    "\n",
    "# Open a window to display the video feed\n",
    "while True:\n",
    "    # Get the video frame\n",
    "    frame = me.get_frame_read().frame\n",
    "\n",
    "    # Resize the frame for better display (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "    # img = cv2.cvtColor(framw, cv2.COLOR_BGR2RGB)   \n",
    "\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Tello Camera\", frame)\n",
    "\n",
    "    \n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    me.send_rc_control(10, 0, 0, 5)\n",
    "\n",
    "    # me.land()\n",
    "\n",
    "    \n",
    "\n",
    "# # To control movement\n",
    "# # me.send_rc_control(left/right, forward/backward, up/down, yaw_velocity)\n",
    "\n",
    "# # To stop - for 5 seconds\n",
    "# me.sleep(60)\n",
    "\n",
    "# # To land\n",
    "# me.land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 49050, Center: (530, 607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:46:42.494 python[8743:124274] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-18 16:46:42.494 python[8743:124274] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 124440, Center: (536, 456)\n",
      "Person moved 151.12 pixels, updating movement.\n",
      "New Drone Position: X=-5.20, Y=-0.06, Z=0.08\n",
      "Area: 183910, Center: (428, 447)\n",
      "Person moved 108.37 pixels, updating movement.\n",
      "New Drone Position: X=-15.80, Y=-0.09, Z=-1.66\n",
      "Area: 172244, Center: (529, 424)\n",
      "Person moved 103.59 pixels, updating movement.\n",
      "New Drone Position: X=-21.35, Y=-0.09, Z=-2.94\n",
      "Area: 125280, Center: (427, 445)\n",
      "Person moved 104.14 pixels, updating movement.\n",
      "New Drone Position: X=-32.00, Y=-0.05, Z=-4.64\n",
      "Area: 142040, Center: (326, 443)\n",
      "Person moved 101.02 pixels, updating movement.\n",
      "New Drone Position: X=-47.70, Y=-0.06, Z=-6.30\n",
      "Area: 130806, Center: (446, 435)\n",
      "Person moved 120.27 pixels, updating movement.\n",
      "New Drone Position: X=-57.40, Y=-0.05, Z=-7.80\n",
      "Area: 123050, Center: (586, 445)\n",
      "Person moved 140.36 pixels, updating movement.\n",
      "New Drone Position: X=-60.10, Y=-0.05, Z=-9.50\n",
      "Area: 71760, Center: (1158, 564)\n",
      "Person moved 584.25 pixels, updating movement.\n",
      "New Drone Position: X=-34.20, Y=0.03, Z=-13.58\n",
      "Area: 217419, Center: (764, 448)\n",
      "Person moved 410.72 pixels, updating movement.\n",
      "New Drone Position: X=-28.00, Y=-0.04, Z=-15.34\n",
      "Area: 140651, Center: (664, 466)\n",
      "Person moved 101.61 pixels, updating movement.\n",
      "New Drone Position: X=-26.80, Y=0.01, Z=-17.46\n",
      "Area: 104580, Center: (1196, 397)\n",
      "Person moved 536.46 pixels, updating movement.\n",
      "New Drone Position: X=1.00, Y=0.05, Z=-18.20\n",
      "Area: 86846, Center: (627, 465)\n",
      "Person moved 573.05 pixels, updating movement.\n",
      "New Drone Position: X=0.35, Y=0.07, Z=-20.30\n",
      "Area: 128674, Center: (1178, 392)\n",
      "Person moved 555.81 pixels, updating movement.\n",
      "New Drone Position: X=27.25, Y=0.04, Z=-20.94\n",
      "Area: 107848, Center: (476, 470)\n",
      "Person moved 706.32 pixels, updating movement.\n",
      "New Drone Position: X=19.05, Y=0.06, Z=-23.14\n",
      "Area: 133042, Center: (1168, 411)\n",
      "Person moved 694.51 pixels, updating movement.\n",
      "New Drone Position: X=45.45, Y=0.04, Z=-24.16\n",
      "Area: 121263, Center: (459, 468)\n",
      "Person moved 711.29 pixels, updating movement.\n",
      "New Drone Position: X=36.40, Y=0.05, Z=-26.32\n",
      "Area: 194036, Center: (559, 443)\n",
      "Person moved 103.08 pixels, updating movement.\n",
      "New Drone Position: X=32.35, Y=0.01, Z=-27.98\n",
      "Area: 283968, Center: (710, 441)\n",
      "Person moved 151.01 pixels, updating movement.\n",
      "New Drone Position: X=35.85, Y=-0.02, Z=-29.60\n",
      "Area: 428970, Center: (523, 442)\n",
      "Person moved 187.00 pixels, updating movement.\n",
      "New Drone Position: X=30.00, Y=-0.06, Z=-31.24\n",
      "Area: 153360, Center: (406, 446)\n",
      "Person moved 117.07 pixels, updating movement.\n",
      "New Drone Position: X=18.30, Y=0.12, Z=-32.96\n",
      "Area: 146076, Center: (506, 455)\n",
      "Person moved 100.40 pixels, updating movement.\n",
      "New Drone Position: X=11.60, Y=0.13, Z=-34.86\n",
      "Area: 303942, Center: (380, 429)\n",
      "Person moved 128.65 pixels, updating movement.\n",
      "New Drone Position: X=-1.40, Y=0.08, Z=-36.24\n",
      "Area: 191642, Center: (480, 430)\n",
      "Person moved 100.00 pixels, updating movement.\n",
      "New Drone Position: X=-9.40, Y=0.13, Z=-37.64\n",
      "Area: 322608, Center: (365, 430)\n",
      "Person moved 115.00 pixels, updating movement.\n",
      "New Drone Position: X=-23.15, Y=0.09, Z=-39.04\n",
      "Area: 199865, Center: (475, 429)\n",
      "Person moved 110.00 pixels, updating movement.\n",
      "New Drone Position: X=-31.40, Y=0.16, Z=-40.42\n",
      "Area: 314148, Center: (374, 429)\n",
      "Person moved 101.00 pixels, updating movement.\n",
      "New Drone Position: X=-44.70, Y=0.12, Z=-41.80\n",
      "Area: 190280, Center: (484, 429)\n",
      "Person moved 110.00 pixels, updating movement.\n",
      "New Drone Position: X=-52.50, Y=0.18, Z=-43.18\n",
      "Area: 302275, Center: (382, 429)\n",
      "Person moved 102.00 pixels, updating movement.\n",
      "New Drone Position: X=-65.40, Y=0.15, Z=-44.56\n",
      "Area: 177784, Center: (488, 428)\n",
      "Person moved 106.00 pixels, updating movement.\n",
      "New Drone Position: X=-73.00, Y=0.22, Z=-45.92\n",
      "Area: 205190, Center: (588, 425)\n",
      "Person moved 100.04 pixels, updating movement.\n",
      "New Drone Position: X=-75.60, Y=0.20, Z=-47.22\n",
      "Area: 94760, Center: (688, 456)\n",
      "Person moved 104.69 pixels, updating movement.\n",
      "New Drone Position: X=-73.20, Y=0.32, Z=-49.14\n",
      "Area: 181656, Center: (792, 453)\n",
      "Person moved 104.04 pixels, updating movement.\n",
      "New Drone Position: X=-65.60, Y=0.27, Z=-51.00\n",
      "Area: 169904, Center: (671, 455)\n",
      "Person moved 121.02 pixels, updating movement.\n",
      "New Drone Position: X=-64.05, Y=0.28, Z=-52.90\n",
      "Area: 106020, Center: (571, 482)\n",
      "Person moved 103.58 pixels, updating movement.\n",
      "New Drone Position: X=-67.50, Y=0.34, Z=-55.34\n",
      "Area: 128184, Center: (493, 419)\n",
      "Person moved 100.26 pixels, updating movement.\n",
      "New Drone Position: X=-74.85, Y=0.32, Z=-56.52\n",
      "Area: 168696, Center: (390, 430)\n",
      "Person moved 103.59 pixels, updating movement.\n",
      "New Drone Position: X=-87.35, Y=0.30, Z=-57.92\n",
      "Area: 124488, Center: (486, 461)\n",
      "Person moved 100.88 pixels, updating movement.\n",
      "New Drone Position: X=-95.05, Y=0.33, Z=-59.94\n",
      "Area: 143072, Center: (590, 451)\n",
      "Person moved 104.48 pixels, updating movement.\n",
      "New Drone Position: X=-97.55, Y=0.32, Z=-61.76\n",
      "Area: 225235, Center: (484, 446)\n",
      "Person moved 106.12 pixels, updating movement.\n",
      "New Drone Position: X=-105.35, Y=0.28, Z=-63.48\n",
      "Area: 163758, Center: (586, 436)\n",
      "Person moved 102.49 pixels, updating movement.\n",
      "New Drone Position: X=-108.05, Y=0.32, Z=-65.00\n",
      "Area: 227664, Center: (753, 450)\n",
      "Person moved 167.59 pixels, updating movement.\n",
      "New Drone Position: X=-102.40, Y=0.29, Z=-66.80\n",
      "Area: 181696, Center: (592, 442)\n",
      "Person moved 161.20 pixels, updating movement.\n",
      "New Drone Position: X=-104.80, Y=0.32, Z=-68.44\n",
      "Area: 193626, Center: (460, 434)\n",
      "Person moved 132.24 pixels, updating movement.\n",
      "New Drone Position: X=-113.80, Y=0.31, Z=-69.92\n",
      "Area: 247506, Center: (340, 422)\n",
      "Person moved 120.60 pixels, updating movement.\n",
      "New Drone Position: X=-128.80, Y=0.29, Z=-71.16\n",
      "Area: 288900, Center: (238, 371)\n",
      "Person moved 114.04 pixels, updating movement.\n",
      "New Drone Position: X=-148.90, Y=0.28, Z=-71.38\n",
      "Area: 287854, Center: (1076, 358)\n",
      "Person moved 838.10 pixels, updating movement.\n",
      "New Drone Position: X=-127.10, Y=0.28, Z=-71.34\n",
      "Area: 295360, Center: (208, 358)\n",
      "Person moved 868.00 pixels, updating movement.\n",
      "New Drone Position: X=-148.70, Y=0.27, Z=-71.30\n",
      "Area: 489190, Center: (935, 357)\n",
      "Person moved 727.00 pixels, updating movement.\n",
      "New Drone Position: X=-133.95, Y=0.23, Z=-71.24\n",
      "Area: 475278, Center: (802, 362)\n",
      "Person moved 133.09 pixels, updating movement.\n",
      "New Drone Position: X=-125.85, Y=0.24, Z=-71.28\n",
      "Area: 477792, Center: (683, 356)\n",
      "Person moved 119.15 pixels, updating movement.\n",
      "New Drone Position: X=-123.70, Y=0.24, Z=-71.20\n",
      "Area: 565006, Center: (784, 361)\n",
      "Person moved 101.12 pixels, updating movement.\n",
      "New Drone Position: X=-116.50, Y=0.22, Z=-71.22\n",
      "Area: 83980, Center: (1147, 427)\n",
      "Person moved 368.95 pixels, updating movement.\n",
      "New Drone Position: X=-91.15, Y=0.79, Z=-72.56\n",
      "Area: 560598, Center: (788, 361)\n",
      "Person moved 365.02 pixels, updating movement.\n",
      "New Drone Position: X=-83.75, Y=0.71, Z=-72.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Inference on the frame \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m model(frame) \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# If a person was detected\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Lists to hold the person centre and area\u001b[39;00m\n\u001b[1;32m     47\u001b[0m myPersonList \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:558\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 558\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:109\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:127\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:148\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    149\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# These values need to be fine tuned\n",
    "K_x = 0.05 # Left/Right movement scale\n",
    "K_y = 0.1  # Forward/Backward movement scale\n",
    "K_z = 0.02 # Up/Down movement scale\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "# Read the drone's initial position\n",
    "drone_x, drone_y, drone_z = 0, 0, 2 \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # Define screen center\n",
    "    frame_center_x = frame.shape[1] // 2  # Middle of frame\n",
    "    frame_center_y = frame.shape[0] // 2 \n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Calculate the change in area\n",
    "            area_change = abs(current_area - last_area)\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                # Left/Right Movement (X-axis)\n",
    "                drone_x += K_x * (person_x - frame_center_x)\n",
    "                # Forward/Backward Movement (Y-axis)\n",
    "                drone_y += K_y * ((last_area / current_area) - 1)\n",
    "                # Up/Down Movement (Z-axis)\n",
    "                drone_z += K_z * (frame_center_y - person_y)\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "                print(f\"New Drone Position: X={drone_x:.2f}, Y={drone_y:.2f}, Z={drone_z:.2f}\")\n",
    "                # Left/Right Movement\n",
    "                # if person_x < frame_center_x - 50:\n",
    "                #     direction = \"left\"\n",
    "                #     print(\"Move Left\")\n",
    "                #     # Send command to drone: move left\n",
    "                # elif person_x > frame_center_x + 50:\n",
    "                #     direction = \"right\"  \n",
    "                #     print(\"Move Right\")\n",
    "                #     # Send command to drone: move right\n",
    "\n",
    "                # # Forward/Backward Movement\n",
    "                # if myPersonListArea[i] < 5000:  # Adjust based on detection area\n",
    "                #     direction = \"forward\"\n",
    "                #     print(\"Move Forward\")\n",
    "                #     # Send command to drone: move forward\n",
    "                # elif myPersonListArea[i] > 15000:\n",
    "                #     direction = \"backward\"\n",
    "                #     print(\"Move Backward\")\n",
    "                #     # Send command to drone: move backward\n",
    "\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS :\n",
    "- If there are multiple people it might change the person it is tracking (Maybe for now I will test it with one person)\n",
    "- It is difficult to find how much the drone should move and I am not sure whether I am doing it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations -- things to check \n",
    "- Latency: Ensure minimal delay between detecting the red spot and sending commands.\n",
    "- Safety: Test in a controlled environment to ensure predictable movements.\n",
    "- Camera Feed Access: If using the drone’s camera feed, ensure you can stream it to your processing device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following a Person"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
