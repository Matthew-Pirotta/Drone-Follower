{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual run that I found the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "\n",
    "- cv2 to capture the image\n",
    "- Yolo to detect the person - include this in litreature review\n",
    "- https://docs.ultralytics.com/models/yolov8/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial things to use the drone - new features\n",
    "# Tello does not work with out model \n",
    "\n",
    "from djitellopy import tello\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "\n",
    "me = tello.Tello()\n",
    "# To connect \n",
    "me.connect()\n",
    "\n",
    "# # To start \n",
    "me.streamon()\n",
    "\n",
    "# # To takeoff\n",
    "# me.takeoff()\n",
    "\n",
    "# Open a window to display the video feed\n",
    "while True:\n",
    "    # Get the video frame\n",
    "    frame = me.get_frame_read().frame\n",
    "\n",
    "    # Resize the frame for better display (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "    # img = cv2.cvtColor(framw, cv2.COLOR_BGR2RGB)   \n",
    "\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Tello Camera\", frame)\n",
    "\n",
    "    \n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    me.send_rc_control(10, 0, 0, 5)\n",
    "\n",
    "    # me.land()\n",
    "\n",
    "    \n",
    "\n",
    "# # To control movement\n",
    "# # me.send_rc_control(left/right, forward/backward, up/down, yaw_velocity)\n",
    "\n",
    "# # To stop - for 5 seconds\n",
    "# me.sleep(60)\n",
    "\n",
    "# # To land\n",
    "# me.land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# These values need to be fine tuned\n",
    "K_x = 0.05 # Left/Right movement scale\n",
    "K_y = 0.1  # Forward/Backward movement scale\n",
    "K_z = 0.02 # Up/Down movement scale\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "# Read the drone's initial position\n",
    "drone_x, drone_y, drone_z = 0, 0, 2 \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # Define screen center\n",
    "    frame_center_x = frame.shape[1] // 2  # Middle of frame\n",
    "    frame_center_y = frame.shape[0] // 2 \n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Calculate the change in area\n",
    "            area_change = abs(current_area - last_area)\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                # Left/Right Movement (X-axis)\n",
    "                drone_x += K_x * (person_x - frame_center_x)\n",
    "                # Forward/Backward Movement (Y-axis)\n",
    "                drone_y += K_y * ((last_area / current_area) - 1)\n",
    "                # Up/Down Movement (Z-axis)\n",
    "                drone_z += K_z * (frame_center_y - person_y)\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "                print(f\"New Drone Position: X={drone_x:.2f}, Y={drone_y:.2f}, Z={drone_z:.2f}\")\n",
    "                # Left/Right Movement\n",
    "                # if person_x < frame_center_x - 50:\n",
    "                #     direction = \"left\"\n",
    "                #     print(\"Move Left\")\n",
    "                #     # Send command to drone: move left\n",
    "                # elif person_x > frame_center_x + 50:\n",
    "                #     direction = \"right\"  \n",
    "                #     print(\"Move Right\")\n",
    "                #     # Send command to drone: move right\n",
    "\n",
    "                # # Forward/Backward Movement\n",
    "                # if myPersonListArea[i] < 5000:  # Adjust based on detection area\n",
    "                #     direction = \"forward\"\n",
    "                #     print(\"Move Forward\")\n",
    "                #     # Send command to drone: move forward\n",
    "                # elif myPersonListArea[i] > 15000:\n",
    "                #     direction = \"backward\"\n",
    "                #     print(\"Move Backward\")\n",
    "                #     # Send command to drone: move backward\n",
    "\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS :\n",
    "- If there are multiple people it might change the person it is tracking (Maybe for now I will test it with one person)\n",
    "- It is difficult to find how much the drone should move and I am not sure whether I am doing it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations -- things to check \n",
    "- Latency: Ensure minimal delay between detecting the red spot and sending commands.\n",
    "- Safety: Test in a controlled environment to ensure predictable movements.\n",
    "- Camera Feed Access: If using the drone’s camera feed, ensure you can stream it to your processing device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following a Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize Deep SORT Tracker \n",
    "tracker = DeepSort(max_age=10, n_init=3)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame \n",
    "    results = model(frame)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls))\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  # Increased from 10 to maintain track through brief occlusions\n",
    "    n_init=5,    # Need 5 consecutive detections to confirm track\n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  # Stricter appearance matching\n",
    "    embedder_model_name=\"osnet_x1_0\",  # Better ReID model\n",
    "    embedder = \"mobilenet\",\n",
    "    half=True  # Use FP16 for faster inference\n",
    ")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    " \n",
    "    # Sharpen the image to make better performance\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))  # Add cropped image\n",
    "\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is very good but a random error came out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=5,    \n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder_model_name=\"osnet_x1_0\",  \n",
    "    embedder=\"mobilenet\",\n",
    "    half=True  \n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image for better detection\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.8:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width = x2 - x1  \n",
    "                height = y2 - y1\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from the tracked person\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Take first detected face\n",
    "\n",
    "                # Check if face is already in database\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    track.track_id = matched_id  # Assign existing ID\n",
    "                else:\n",
    "                    face_db[track_id] = face_encoding  # Save new face\n",
    "\n",
    "        # Draw tracking box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track.track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker with Default Embedder\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",  # Use default DeepSORT embedder\n",
    "    half=True,\n",
    "    embedder_gpu=True  # Enable GPU if available\n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "# {track_id: unique_id}\n",
    "person_id_map = {} \n",
    "# Counter for unique IDs \n",
    "next_person_id = 1 \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width, height = x2 - x1, y2 - y1  \n",
    "\n",
    "                # Extract cropped image for embedding\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue  # Avoid invalid crops\n",
    "\n",
    "                area = width * height \n",
    "\n",
    "                # Append detection with an empty embedding (DeepSORT will handle it)\n",
    "                # Default 128-dim zero vector\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker with valid detections\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb()) \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                # Take first detected face\n",
    "                face_encoding = face_encodings[0]  \n",
    "\n",
    "                # Match face to existing IDs\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    # Assign stored ID / the same ID as before\n",
    "                    person_id_map[track_id] = matched_id  \n",
    "                else:\n",
    "                    # New person, assign a unique ID\n",
    "                    face_db[next_person_id] = face_encoding  \n",
    "                    person_id_map[track_id] = next_person_id  \n",
    "                    next_person_id += 1    \n",
    "\n",
    "        # Get the unique ID for this person\n",
    "        unique_id = person_id_map.get(track_id, track_id)\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {unique_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)    \n",
    "    \n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "face_db = {}  # Stores face encodings {unique_id: face_encoding}\n",
    "person_id_map = {}  # Maps track_id → unique_id\n",
    "next_person_id = 1\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            conf = box.conf[0].item()\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                \n",
    "                # Extract person crop\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Will store the final ID\n",
    "\n",
    "        if face_locations:\n",
    "            # Adjust coordinates to global frame\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use the first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.4)\n",
    "                    if match[0]:  # Found a match\n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    next_person_id += 1\n",
    "\n",
    "                # Update the mapping to the current track_id\n",
    "                person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # If no face was detected, check if we already assigned an ID to this track\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id, None)\n",
    "\n",
    "        # If track_id is new and has no face, assign a temporary unique ID\n",
    "        if assigned_id is None:\n",
    "            assigned_id = next_person_id\n",
    "            person_id_map[track_id] = assigned_id\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code for tracking all the persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "# Stores face encodings {unique_id: face_encoding}\n",
    "face_db = {}  \n",
    "# Maps track_id → unique_id\n",
    "person_id_map = {}  \n",
    "# Keeps track of assigned person IDs\n",
    "used_person_ids = set()  \n",
    "next_person_id = 1\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame \n",
    "    results = model(sharpened)\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0])\n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()\n",
    "            # Class 0 = \"person\", confidence > 70%\n",
    "            if cls == 0 and conf > 0.7:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                #Calculate the width and the height of the bunding box\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                # Calculate area\n",
    "                area = width * height\n",
    "\n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "                # Extract person crop\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Will store the final ID\n",
    "\n",
    "        if face_locations:\n",
    "            # Adjust coordinates to global frame\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use the first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.55)\n",
    "                    if match[0]:  # Found a match\n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    while next_person_id in used_person_ids:  # Ensure unique ID assignment\n",
    "                        next_person_id += 1\n",
    "\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    used_person_ids.add(next_person_id)\n",
    "                    next_person_id += 1\n",
    "\n",
    "        # Ensure a stable mapping for DeepSORT track_id\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id)\n",
    "\n",
    "        if assigned_id is None:\n",
    "            # Assign a new unique person ID only if no previous ID exists\n",
    "            while next_person_id in used_person_ids:\n",
    "                next_person_id += 1\n",
    "\n",
    "            assigned_id = next_person_id\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Store the mapping between DeepSORT track_id and the stable person_id\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code for tracking only the persons with id 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "face_db = {}  # Stores face encodings {unique_id: face_encoding}\n",
    "person_id_map = {}  # Maps track_id → unique_id\n",
    "used_person_ids = set()  # Keeps track of assigned person IDs\n",
    "next_person_id = 1\n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100  # Movement sensitivity\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # List to hold detections\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  # Get class ID\n",
    "            conf = box.conf[0].item()  # Confidence score\n",
    "\n",
    "            # Class 0 = \"person\", confidence > 70%\n",
    "            if cls == 0 and conf > 0.7:  \n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    # Track only ID=1\n",
    "    person_x, person_y, current_area = None, None, 0\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Final ID assignment\n",
    "\n",
    "        if face_locations:\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.55)\n",
    "                    if match[0]:  \n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    while next_person_id in used_person_ids:  \n",
    "                        next_person_id += 1\n",
    "\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    used_person_ids.add(next_person_id)\n",
    "                    next_person_id += 1\n",
    "\n",
    "        # Ensure stable ID mapping\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id)\n",
    "\n",
    "        if assigned_id is None:\n",
    "            while next_person_id in used_person_ids:\n",
    "                next_person_id += 1\n",
    "\n",
    "            assigned_id = next_person_id\n",
    "            used_person_ids.add(next_person_id)\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Store mapping\n",
    "        person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # Only track and draw for ID = 1\n",
    "        if assigned_id == 1:\n",
    "            person_x, person_y, current_area = cx, cy, area\n",
    "\n",
    "            # Draw tracking box **only for ID = 1**\n",
    "            cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Process movement of ID=1\n",
    "    if person_x is not None and person_y is not None:\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'ID=1 -> Area: {current_area}, Center: ({person_x}, {person_y})')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEMS ENCOUNTERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep sort causing different bounding area of the same person given different ids.\n",
    "- If a person goes missing and comes back you end up with a different ID.\n",
    "- If I cover the camera and uncover it I get an error but this should not be this case in our assignnment as the drone should never loose track of the person.\n",
    "- Tuning the tolerance of face detection was very difficult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
