{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual run that I found the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "\n",
    "- cv2 to capture the image\n",
    "- Yolo to detect the person - include this in litreature review\n",
    "- https://docs.ultralytics.com/models/yolov8/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial things to use the drone - new features\n",
    "# Tello does not work with out model \n",
    "\n",
    "from djitellopy import tello\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "\n",
    "me = tello.Tello()\n",
    "# To connect \n",
    "me.connect()\n",
    "\n",
    "# # To start \n",
    "me.streamon()\n",
    "\n",
    "# # To takeoff\n",
    "# me.takeoff()\n",
    "\n",
    "# Open a window to display the video feed\n",
    "while True:\n",
    "    # Get the video frame\n",
    "    frame = me.get_frame_read().frame\n",
    "\n",
    "    # Resize the frame for better display (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "    # img = cv2.cvtColor(framw, cv2.COLOR_BGR2RGB)   \n",
    "\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Tello Camera\", frame)\n",
    "\n",
    "    \n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    me.send_rc_control(10, 0, 0, 5)\n",
    "\n",
    "    # me.land()\n",
    "\n",
    "    \n",
    "\n",
    "# # To control movement\n",
    "# # me.send_rc_control(left/right, forward/backward, up/down, yaw_velocity)\n",
    "\n",
    "# # To stop - for 5 seconds\n",
    "# me.sleep(60)\n",
    "\n",
    "# # To land\n",
    "# me.land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# These values need to be fine tuned\n",
    "K_x = 0.05 # Left/Right movement scale\n",
    "K_y = 0.1  # Forward/Backward movement scale\n",
    "K_z = 0.02 # Up/Down movement scale\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "# Read the drone's initial position\n",
    "drone_x, drone_y, drone_z = 0, 0, 2 \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # Define screen center\n",
    "    frame_center_x = frame.shape[1] // 2  # Middle of frame\n",
    "    frame_center_y = frame.shape[0] // 2 \n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Calculate the change in area\n",
    "            area_change = abs(current_area - last_area)\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                # Left/Right Movement (X-axis)\n",
    "                drone_x += K_x * (person_x - frame_center_x)\n",
    "                # Forward/Backward Movement (Y-axis)\n",
    "                drone_y += K_y * ((last_area / current_area) - 1)\n",
    "                # Up/Down Movement (Z-axis)\n",
    "                drone_z += K_z * (frame_center_y - person_y)\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "                print(f\"New Drone Position: X={drone_x:.2f}, Y={drone_y:.2f}, Z={drone_z:.2f}\")\n",
    "                # Left/Right Movement\n",
    "                # if person_x < frame_center_x - 50:\n",
    "                #     direction = \"left\"\n",
    "                #     print(\"Move Left\")\n",
    "                #     # Send command to drone: move left\n",
    "                # elif person_x > frame_center_x + 50:\n",
    "                #     direction = \"right\"  \n",
    "                #     print(\"Move Right\")\n",
    "                #     # Send command to drone: move right\n",
    "\n",
    "                # # Forward/Backward Movement\n",
    "                # if myPersonListArea[i] < 5000:  # Adjust based on detection area\n",
    "                #     direction = \"forward\"\n",
    "                #     print(\"Move Forward\")\n",
    "                #     # Send command to drone: move forward\n",
    "                # elif myPersonListArea[i] > 15000:\n",
    "                #     direction = \"backward\"\n",
    "                #     print(\"Move Backward\")\n",
    "                #     # Send command to drone: move backward\n",
    "\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS :\n",
    "- If there are multiple people it might change the person it is tracking (Maybe for now I will test it with one person)\n",
    "- It is difficult to find how much the drone should move and I am not sure whether I am doing it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations -- things to check \n",
    "- Latency: Ensure minimal delay between detecting the red spot and sending commands.\n",
    "- Safety: Test in a controlled environment to ensure predictable movements.\n",
    "- Camera Feed Access: If using the drone’s camera feed, ensure you can stream it to your processing device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following a Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more accurate - it tracks every part of a person\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)  \n",
    "\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                bbox_height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * bbox_height\n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "                # Append the area and the center of the circle\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "\n",
    "        # To avoid having constant outputs for example when the person moves only a bit \n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            # Calculate distance moved in pixels\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            # Calculate the distance moved\n",
    "            # Euclidean distance \n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "            # Check if the person moved more than the threshold to avoid having constant moves\n",
    "            # This is only done to make the movement smoother - this was being calculated even if I barely move which we do not want \n",
    "            # We do not want to have a drone that would crash having constant changes in the position \n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "                print(f\"Person moved {distance_moved:.2f} pixels, updating movement.\")\n",
    "\n",
    "                # Update last position\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        # Track the initial position of the person - assuming this is the space you want to have between the person and the drone \n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            # First detection, initialize last position\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize Deep SORT Tracker \n",
    "tracker = DeepSort(max_age=10, n_init=3)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame \n",
    "    results = model(frame)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls))\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Added verbose = False to avoid having a lot of outputs when running the code for example this was outputting for each small detection\n",
    "# 0: 384x640 1 person, 71.6ms\n",
    "# Speed: 9.2ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  # Increased from 10 to maintain track through brief occlusions\n",
    "    n_init=5,    # Need 5 consecutive detections to confirm track\n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  # Stricter appearance matching\n",
    "    embedder_model_name=\"osnet_x1_0\",  # Better ReID model\n",
    "    embedder = \"mobilenet\",\n",
    "    half=True  # Use FP16 for faster inference\n",
    ")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "# While the webcam is open\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    " \n",
    "    # Sharpen the image to make better performance\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    # Run YOLOv8 inference\n",
    "    # Inference on the frame\n",
    "    results = model(sharpened)\n",
    "\n",
    "    # Prepare Deep Sort input format\n",
    "    detections = []  \n",
    "    # If a person was detected\n",
    "    # Lists to hold the person centre and area\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID and confidence score\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item() \n",
    "\n",
    "            # Detect only people with confidence score of 0.8 or more\n",
    "            if cls == 0 and conf > 0.8:  # Detect only people\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                width = x2 - x1 \n",
    "                height = y2 - y1\n",
    "                # Calculate area\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))  # Add cropped image\n",
    "\n",
    "\n",
    "    # Update Deep SORT Tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            # Ignore unconfirmed tracks\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Append to lists\n",
    "        # Append the area and the center of the circle\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + Deep SORT Tracking\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit on 'q'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is very good but a random error came out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model (Nano version)\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=5,    \n",
    "    max_iou_distance=0.4,\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder_model_name=\"osnet_x1_0\",  \n",
    "    embedder=\"mobilenet\",\n",
    "    half=True  \n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image for better detection\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.8:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width = x2 - x1  \n",
    "                height = y2 - y1\n",
    "                area = width * height  \n",
    "\n",
    "                detections.append(([x1, y1, width, height], conf, cls, frame[y1:y2, x1:x2]))\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())  \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from the tracked person\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Take first detected face\n",
    "\n",
    "                # Check if face is already in database\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    track.track_id = matched_id  # Assign existing ID\n",
    "                else:\n",
    "                    face_db[track_id] = face_encoding  # Save new face\n",
    "\n",
    "        # Draw tracking box and ID\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "        cv2.putText(sharpened, f\"ID: {track.track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 522500, Center: (716, 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:24:27.328 python[26355:224239] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-19 16:24:27.328 python[26355:224239] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 551400, Center: (589, 412)\n",
      "Area: 610038, Center: (496, 350)\n",
      "Area: 179104, Center: (396, 307)\n",
      "Area: 179644, Center: (295, 264)\n",
      "Area: 161640, Center: (194, 220)\n",
      "Area: 136416, Center: (93, 177)\n",
      "Area: 217152, Center: (675, 481)\n",
      "Area: 148338, Center: (573, 510)\n",
      "Area: 358750, Center: (660, 411)\n",
      "Area: 293400, Center: (736, 484)\n",
      "Area: 436224, Center: (829, 529)\n",
      "Area: 616745, Center: (593, 356)\n",
      "Area: 551950, Center: (477, 381)\n",
      "Area: 448350, Center: (368, 408)\n",
      "Area: 391368, Center: (278, 356)\n",
      "Area: 247779, Center: (175, 355)\n",
      "Area: 380380, Center: (75, 345)\n",
      "Area: 383056, Center: (649, 448)\n",
      "Area: 448987, Center: (685, 349)\n",
      "Area: 750127, Center: (562, 369)\n",
      "Area: 414261, Center: (622, 450)\n",
      "Area: 274744, Center: (484, 469)\n",
      "Area: 314360, Center: (374, 448)\n",
      "Area: 267718, Center: (259, 428)\n",
      "Area: 441504, Center: (395, 388)\n",
      "Area: 355488, Center: (649, 444)\n",
      "Area: 374710, Center: (763, 466)\n",
      "Area: 374710, Center: (884, 490)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m sharpened \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfilter2D(frame, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Run YOLOv8 inference\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m results \u001b[38;5;241m=\u001b[39m model(sharpened)\n\u001b[1;32m     56\u001b[0m detections \u001b[38;5;241m=\u001b[39m []  \n\u001b[1;32m     57\u001b[0m myPersonList \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:558\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 558\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:109\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:127\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:148\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    149\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:348\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker with Default Embedder\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",  # Use default DeepSORT embedder\n",
    "    half=True,\n",
    "    embedder_gpu=True  # Enable GPU if available\n",
    ")\n",
    "\n",
    "# Face Database for ID assignment\n",
    "face_db = {}\n",
    "# {track_id: unique_id}\n",
    "person_id_map = {} \n",
    "# Counter for unique IDs \n",
    "next_person_id = 1 \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Tracking history\n",
    "last_position = None\n",
    "last_area = None\n",
    "pixel_threshold = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []  \n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])  \n",
    "            conf = box.conf[0].item()  \n",
    "\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "                width, height = x2 - x1, y2 - y1  \n",
    "\n",
    "                # Extract cropped image for embedding\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue  # Avoid invalid crops\n",
    "\n",
    "                area = width * height \n",
    "\n",
    "                # Append detection with an empty embedding (DeepSORT will handle it)\n",
    "                # Default 128-dim zero vector\n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  \n",
    "\n",
    "    # Update DeepSORT tracker with valid detections\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  \n",
    "\n",
    "        track_id = track.track_id  \n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb()) \n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        # Detect face in person crop\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "\n",
    "        if face_locations:\n",
    "            # Convert face locations to global frame coordinates\n",
    "            adjusted_faces = [(y1+top, x1+right, y1+bottom, x1+left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                # Take first detected face\n",
    "                face_encoding = face_encodings[0]  \n",
    "\n",
    "                # Match face to existing IDs\n",
    "                matched_id = None\n",
    "                for saved_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  \n",
    "                        matched_id = saved_id\n",
    "                        break\n",
    "\n",
    "                if matched_id:\n",
    "                    # Assign stored ID / the same ID as before\n",
    "                    person_id_map[track_id] = matched_id  \n",
    "                else:\n",
    "                    # New person, assign a unique ID\n",
    "                    face_db[next_person_id] = face_encoding  \n",
    "                    person_id_map[track_id] = next_person_id  \n",
    "                    next_person_id += 1    \n",
    "\n",
    "        # Get the unique ID for this person\n",
    "        unique_id = person_id_map.get(track_id, track_id)\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {unique_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Store position & area for tracking movement\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)    \n",
    "    \n",
    "    # Track the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        person_x, person_y = myPersonList[i]        \n",
    "        current_area = myPersonListArea[i]\n",
    "\n",
    "        if last_position is not None and last_area is not None and current_area > 0:\n",
    "            dx = person_x - last_position[0]\n",
    "            dy = person_y - last_position[1]\n",
    "            distance_moved = math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance_moved > pixel_threshold:\n",
    "                print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "                last_position = (person_x, person_y)\n",
    "                last_area = current_area\n",
    "        else:\n",
    "            print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "            last_position = (person_x, person_y)\n",
    "            last_area = current_area\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m sharpened \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfilter2D(frame, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Run YOLOv8 inference\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m results \u001b[38;5;241m=\u001b[39m model(sharpened)\n\u001b[1;32m     49\u001b[0m detections \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:268\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/models/yolo/detect/predict.py:25\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mnon_max_suppression(\n\u001b[1;32m     26\u001b[0m         preds,\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mconf,\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39miou,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mclasses,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39magnostic_nms,\n\u001b[1;32m     31\u001b[0m         max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_det,\n\u001b[1;32m     32\u001b[0m         nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames),\n\u001b[1;32m     33\u001b[0m         end2end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend2end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m         rotated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/ops.py:284\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated, end2end)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Detections matrix nx6 (xyxy, conf, cls)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m box, \u001b[38;5;28mcls\u001b[39m, mask \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;241m4\u001b[39m, nc, nm), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_label:\n\u001b[1;32m    287\u001b[0m     i, j \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m>\u001b[39m conf_thres)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:1028\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit_with_sizes(\u001b[38;5;28mself\u001b[39m, split_size, dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_VF.py:27\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Suppress YOLOv8 logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\", verbose=False)\n",
    "\n",
    "# Initialize DeepSORT Tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,  \n",
    "    n_init=3,    \n",
    "    max_iou_distance=0.5,\n",
    "    max_cosine_distance=0.4,  \n",
    "    embedder_model_name=\"mobilenetv2\",\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Persistent ID tracking\n",
    "face_db = {}  # Stores face encodings {unique_id: face_encoding}\n",
    "person_id_map = {}  # Maps track_id → unique_id\n",
    "next_person_id = 1\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Sharpen image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame, -1, kernel)\n",
    "    \n",
    "    # Run YOLOv8 inference\n",
    "    results = model(sharpened)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            conf = box.conf[0].item()\n",
    "            if cls == 0 and conf > 0.7:  # Only detect people\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                \n",
    "                # Extract person crop\n",
    "                person_crop = sharpened[y1:y2, x1:x2]\n",
    "                if person_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                detections.append(([x1, y1, width, height], conf, 0, np.zeros((128,))))  # Default 128-dim zero vector\n",
    "\n",
    "    # Update DeepSORT tracker\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=sharpened)\n",
    "\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Extract face from person crop\n",
    "        person_crop = sharpened[y1:y2, x1:x2]\n",
    "        face_locations = face_recognition.face_locations(person_crop, model=\"hog\")\n",
    "        \n",
    "        assigned_id = None  # Will store the final ID\n",
    "\n",
    "        if face_locations:\n",
    "            # Adjust coordinates to global frame\n",
    "            adjusted_faces = [(y1 + top, x1 + right, y1 + bottom, x1 + left) for (top, right, bottom, left) in face_locations]\n",
    "\n",
    "            # Extract face encodings\n",
    "            face_encodings = face_recognition.face_encodings(sharpened, known_face_locations=adjusted_faces)\n",
    "\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]  # Use the first detected face\n",
    "\n",
    "                # Check if this face matches a known person\n",
    "                for person_id, saved_encoding in face_db.items():\n",
    "                    match = face_recognition.compare_faces([saved_encoding], face_encoding, tolerance=0.6)\n",
    "                    if match[0]:  # Found a match\n",
    "                        assigned_id = person_id\n",
    "                        break\n",
    "\n",
    "                # If no match, assign a new unique ID\n",
    "                if assigned_id is None:\n",
    "                    assigned_id = next_person_id\n",
    "                    face_db[next_person_id] = face_encoding\n",
    "                    next_person_id += 1\n",
    "\n",
    "                # Update the mapping to the current track_id\n",
    "                person_id_map[track_id] = assigned_id\n",
    "\n",
    "        # If no face was detected, check if we already assigned an ID to this track\n",
    "        if assigned_id is None:\n",
    "            assigned_id = person_id_map.get(track_id, None)\n",
    "\n",
    "        # If track_id is new and has no face, assign a temporary unique ID\n",
    "        if assigned_id is None:\n",
    "            assigned_id = next_person_id\n",
    "            person_id_map[track_id] = assigned_id\n",
    "            next_person_id += 1\n",
    "\n",
    "        # Draw tracking box\n",
    "        cv2.rectangle(sharpened, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(sharpened, f\"ID: {assigned_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(sharpened, (cx, cy), 5, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 + DeepSORT + Face Recognition\", sharpened)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEMS ENCOUNTERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep sort causing different bounding area of the same person given different ids.\n",
    "- If a person goes missing and comes back you end up with a different ID.\n",
    "- If I cover the camera and uncover it I get an error but this should not be this case in our assignnment as the drone should never loose track of the person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
