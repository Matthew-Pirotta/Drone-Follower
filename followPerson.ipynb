{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial things to use the drone\n",
    "\n",
    "# from djitellopy import tello\n",
    "# from time import sleep\n",
    "\n",
    "# me = tello.Tello()\n",
    "# To connect \n",
    "# me.connect()\n",
    "\n",
    "# To takeoff\n",
    "#Â me.takeoff()\n",
    "\n",
    "# To control movement\n",
    "# me.send_rc_control(left/right, forward/backward, up/down, yaw_velocity)\n",
    "\n",
    "# To stop\n",
    "# sleep(5)\n",
    "\n",
    "# To land\n",
    "# me.land()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litreature \n",
    "- video of drone Face tracking part \n",
    "https://www.youtube.com/watch?v=LmEcyQnfpDA\n",
    "\n",
    "- cv2 github repo to fetch the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE TRACKING - this works very well\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Detecting a face\n",
    "def findFace(img):\n",
    "    faseCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faseCascade.detectMultiScale(imgGray, 1.2, 8)\n",
    "\n",
    "    myFaceList = []\n",
    "    myFaceListArea = []\n",
    "\n",
    "    for(x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cx = x + w//2\n",
    "        cy = y + h//2\n",
    "        area = w*h\n",
    "        cv2.circle(img, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "        myFaceList.append((cx, cy, area))\n",
    "        myFaceListArea.append(area)\n",
    "    \n",
    "    if len(myFaceList)!= 0:\n",
    "        i = myFaceListArea.index(max(myFaceListArea))\n",
    "        return img, [myFaceList[i], myFaceListArea[i]]\n",
    "    \n",
    "    # Default return to prevent errors\n",
    "    return img, [[0, 0], 0]\n",
    "    \n",
    "# def trackFace(me, info, w, pidf, pError):\n",
    "#     return\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, img = cap.read()\n",
    "    img, info = findFace(img)\n",
    "    print(f'Area: {info[1]}, Center: {info[0]}')\n",
    "    cv2.imshow(\"Face Tracking\", img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 7840, Center: (811, 462)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 21976, Center: (134, 231)\n",
      "Area: 8466, Center: (808, 463)\n",
      "Area: 24252, Center: (132, 224)\n",
      "Area: 21976, Center: (131, 232)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 24708, Center: (134, 225)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 30144, Center: (128, 221)\n",
      "Area: 22110, Center: (132, 226)\n",
      "Area: 24252, Center: (131, 224)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 21976, Center: (129, 226)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 9810, Center: (813, 463)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 24080, Center: (134, 231)\n",
      "Area: 21976, Center: (133, 232)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 20382, Center: (132, 232)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 25810, Center: (131, 229)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 23322, Center: (132, 225)\n",
      "Area: 2142, Center: (406, 552)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 28611, Center: (134, 225)\n",
      "Area: 28952, Center: (130, 226)\n",
      "Area: 33165, Center: (128, 221)\n",
      "Area: 22576, Center: (135, 229)\n",
      "Area: 45548, Center: (607, 200)\n",
      "Area: 3740, Center: (423, 441)\n",
      "Area: 4380, Center: (420, 442)\n",
      "Area: 27450, Center: (130, 224)\n",
      "Area: 4712, Center: (422, 442)\n",
      "Area: 35360, Center: (601, 212)\n",
      "Area: 45784, Center: (597, 213)\n",
      "Area: 23322, Center: (128, 228)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4307, Center: (421, 441)\n",
      "Area: 28764, Center: (602, 210)\n",
      "Area: 34068, Center: (614, 165)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 32800, Center: (674, 144)\n",
      "Area: 37840, Center: (681, 110)\n",
      "Area: 19968, Center: (238, 141)\n",
      "Area: 45784, Center: (664, 106)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 43240, Center: (643, 112)\n",
      "Area: 45120, Center: (643, 114)\n",
      "Area: 41400, Center: (640, 122)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 48800, Center: (643, 132)\n",
      "Area: 52832, Center: (639, 137)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 58692, Center: (640, 162)\n",
      "Area: 20253, Center: (241, 170)\n",
      "Area: 17908, Center: (239, 172)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 22879, Center: (238, 169)\n",
      "Area: 20253, Center: (241, 171)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 19404, Center: (240, 173)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 64120, Center: (636, 154)\n",
      "Area: 22879, Center: (239, 167)\n",
      "Area: 17908, Center: (240, 174)\n",
      "Area: 64630, Center: (634, 160)\n",
      "Area: 23630, Center: (236, 166)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 25665, Center: (235, 158)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 26460, Center: (240, 162)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 22879, Center: (235, 167)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 59670, Center: (623, 154)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 53760, Center: (625, 149)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 62879, Center: (628, 148)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 48357, Center: (614, 159)\n",
      "Area: 52832, Center: (591, 149)\n",
      "Area: 45978, Center: (526, 142)\n",
      "Area: 21679, Center: (238, 185)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 14850, Center: (645, 576)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4575, Center: (792, 238)\n",
      "Area: 6734, Center: (791, 228)\n",
      "Area: 121275, Center: (204, 268)\n",
      "Area: 8944, Center: (439, 512)\n",
      "Area: 133724, Center: (208, 273)\n",
      "Area: 10396, Center: (704, 540)\n",
      "Area: 6177, Center: (792, 234)\n",
      "Area: 5712, Center: (792, 233)\n",
      "Area: 3685, Center: (212, 225)\n",
      "Area: 6900, Center: (794, 232)\n",
      "Area: 5644, Center: (793, 233)\n",
      "Area: 121275, Center: (199, 268)\n",
      "Area: 4914, Center: (756, 335)\n",
      "Area: 3864, Center: (212, 226)\n",
      "Area: 5712, Center: (213, 227)\n",
      "Area: 4248, Center: (213, 225)\n",
      "Area: 60162, Center: (503, 129)\n",
      "Area: 3445, Center: (212, 226)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4774, Center: (793, 233)\n",
      "Area: 11020, Center: (207, 398)\n",
      "Area: 3685, Center: (212, 224)\n",
      "Area: 4774, Center: (212, 226)\n",
      "Area: 7840, Center: (793, 231)\n",
      "Area: 3564, Center: (212, 226)\n",
      "Area: 4992, Center: (212, 227)\n",
      "Area: 6020, Center: (794, 233)\n",
      "Area: 6177, Center: (212, 226)\n",
      "Area: 20382, Center: (207, 410)\n",
      "Area: 4307, Center: (213, 225)\n",
      "Area: 3990, Center: (212, 225)\n",
      "Area: 4440, Center: (213, 226)\n",
      "Area: 18848, Center: (206, 408)\n",
      "Area: 3276, Center: (212, 226)\n",
      "Area: 6660, Center: (211, 227)\n",
      "Area: 18450, Center: (204, 406)\n",
      "Area: 5712, Center: (793, 232)\n",
      "Area: 14740, Center: (204, 404)\n",
      "Area: 14985, Center: (206, 404)\n",
      "Area: 6336, Center: (795, 233)\n",
      "Area: 5346, Center: (793, 233)\n",
      "Area: 25344, Center: (203, 406)\n",
      "Area: 17255, Center: (205, 403)\n",
      "Area: 16215, Center: (208, 403)\n",
      "Area: 7068, Center: (793, 231)\n",
      "Area: 6020, Center: (793, 233)\n",
      "Area: 11232, Center: (206, 400)\n",
      "Area: 459750, Center: (494, 391)\n",
      "Area: 15846, Center: (206, 403)\n",
      "Area: 6177, Center: (211, 226)\n",
      "Area: 11979, Center: (206, 398)\n",
      "Area: 22410, Center: (205, 405)\n",
      "Area: 23940, Center: (206, 406)\n",
      "Area: 24252, Center: (205, 408)\n",
      "Area: 120576, Center: (198, 270)\n",
      "Area: 3864, Center: (212, 227)\n",
      "Area: 3864, Center: (213, 226)\n",
      "Area: 5644, Center: (792, 232)\n",
      "Area: 3990, Center: (213, 225)\n",
      "Area: 116116, Center: (198, 263)\n",
      "Area: 112110, Center: (198, 268)\n",
      "Area: 21252, Center: (205, 409)\n",
      "Area: 20253, Center: (206, 404)\n",
      "Area: 5412, Center: (794, 232)\n",
      "Area: 116116, Center: (198, 270)\n",
      "Area: 6660, Center: (212, 225)\n",
      "Area: 15846, Center: (206, 403)\n",
      "Area: 16992, Center: (205, 404)\n",
      "Area: 16992, Center: (205, 406)\n",
      "Area: 15846, Center: (203, 405)\n",
      "Area: 4248, Center: (211, 225)\n",
      "Area: 7238, Center: (212, 225)\n",
      "Area: 21384, Center: (204, 403)\n",
      "Area: 19125, Center: (204, 403)\n",
      "Area: 4914, Center: (793, 232)\n",
      "Area: 6336, Center: (793, 232)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4992, Center: (213, 227)\n",
      "Area: 6177, Center: (774, 533)\n",
      "Area: 6177, Center: (794, 231)\n",
      "Area: 5865, Center: (793, 231)\n",
      "Area: 3213, Center: (212, 224)\n",
      "Area: 128628, Center: (201, 264)\n",
      "Area: 6336, Center: (793, 230)\n",
      "Area: 116116, Center: (195, 267)\n",
      "Area: 15960, Center: (206, 404)\n",
      "Area: 5200, Center: (212, 227)\n",
      "Area: 17374, Center: (207, 400)\n",
      "Area: 5796, Center: (793, 233)\n",
      "Area: 116802, Center: (198, 266)\n",
      "Area: 110768, Center: (204, 271)\n",
      "Area: 6020, Center: (792, 233)\n",
      "Area: 5346, Center: (794, 233)\n",
      "Area: 5796, Center: (794, 232)\n",
      "Area: 11979, Center: (758, 400)\n",
      "Area: 108472, Center: (200, 263)\n",
      "Area: 9222, Center: (437, 372)\n",
      "Area: 5712, Center: (792, 232)\n",
      "Area: 6020, Center: (793, 231)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4514, Center: (211, 226)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4440, Center: (212, 226)\n",
      "Area: 4712, Center: (213, 226)\n",
      "Area: 3740, Center: (212, 225)\n",
      "Area: 3564, Center: (212, 226)\n",
      "Area: 3100, Center: (212, 224)\n",
      "Area: 3564, Center: (213, 225)\n",
      "Area: 2784, Center: (211, 224)\n",
      "Area: 3864, Center: (210, 226)\n",
      "Area: 3564, Center: (211, 225)\n",
      "Area: 4575, Center: (209, 225)\n",
      "Area: 3685, Center: (212, 224)\n",
      "Area: 3445, Center: (211, 225)\n",
      "Area: 38626, Center: (749, 532)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 17640, Center: (209, 404)\n",
      "Area: 18450, Center: (191, 408)\n",
      "Area: 3990, Center: (383, 562)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 9030, Center: (365, 433)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 5200, Center: (365, 431)\n",
      "Area: 5346, Center: (367, 433)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 5712, Center: (365, 431)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 8383, Center: (367, 434)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 4712, Center: (366, 432)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 7488, Center: (366, 435)\n",
      "Area: 3100, Center: (366, 430)\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 0, Center: [0, 0]\n",
      "Area: 3990, Center: (333, 670)\n",
      "Area: 3808, Center: (332, 670)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     _, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 32\u001b[0m     img, info \u001b[38;5;241m=\u001b[39m findPerson(img)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArea: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Center: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerson Detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mfindPerson\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindPerson\u001b[39m(img):\n\u001b[0;32m----> 6\u001b[0m     personCascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaarcascade_upperbody.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     imgGray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m      8\u001b[0m     persons \u001b[38;5;241m=\u001b[39m personCascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(imgGray, \u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Adjust scaleFactor & minNeighbors if needed\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is detecting clocks as well as people\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Detecting a person\n",
    "def findPerson(img):\n",
    "    personCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_upperbody.xml\")\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    persons = personCascade.detectMultiScale(imgGray, 1.1, 4)  # Adjust scaleFactor & minNeighbors if needed\n",
    "\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for (x, y, w, h) in persons:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cx = x + w // 2\n",
    "        cy = y + h // 2\n",
    "        area = w * h\n",
    "        cv2.circle(img, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "        myPersonList.append((cx, cy))\n",
    "        myPersonListArea.append(area)\n",
    "\n",
    "    if len(myPersonList) != 0:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))  # Select largest detected person\n",
    "        return img, [myPersonList[i], myPersonListArea[i]]\n",
    "\n",
    "    return img, [[0, 0], 0]  # Default return to prevent errors\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, img = cap.read()\n",
    "    img, info = findPerson(img)\n",
    "    print(f'Area: {info[1]}, Center: {info[0]}')\n",
    "\n",
    "    cv2.imshow(\"Person Detection\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "\n",
    "- cv2 to capture the image\n",
    "- Yolo to detect the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 4.5ms preprocess, 57.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419352, Center: (666, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 1.0ms preprocess, 47.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422625, Center: (663, 447)\n",
      "\n",
      "0: 384x640 1 person, 48.3ms\n",
      "Speed: 1.4ms preprocess, 48.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422820, Center: (667, 449)\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.5ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417276, Center: (662, 450)\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415896, Center: (665, 452)\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 1.4ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415668, Center: (665, 450)\n",
      "\n",
      "0: 384x640 1 person, 50.8ms\n",
      "Speed: 8.5ms preprocess, 50.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 393000, Center: (632, 449)\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420772, Center: (663, 448)\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 1.8ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420019, Center: (660, 446)\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 1.7ms preprocess, 44.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422378, Center: (659, 447)\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 1.2ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413400, Center: (656, 449)\n",
      "\n",
      "0: 384x640 2 persons, 52.7ms\n",
      "Speed: 1.6ms preprocess, 52.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 418170, Center: (652, 447)\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 1.3ms preprocess, 49.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 426536, Center: (666, 448)\n",
      "\n",
      "0: 384x640 1 person, 55.5ms\n",
      "Speed: 1.6ms preprocess, 55.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420772, Center: (660, 448)\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 1.5ms preprocess, 52.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419475, Center: (658, 447)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.7ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416056, Center: (659, 448)\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 1.9ms preprocess, 48.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420210, Center: (661, 447)\n",
      "\n",
      "0: 384x640 1 person, 52.6ms\n",
      "Speed: 2.0ms preprocess, 52.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417040, Center: (662, 449)\n",
      "\n",
      "0: 384x640 1 person, 57.3ms\n",
      "Speed: 1.3ms preprocess, 57.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413153, Center: (655, 448)\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 1.1ms preprocess, 58.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416635, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 54.7ms\n",
      "Speed: 1.2ms preprocess, 54.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 381324, Center: (625, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 1.5ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414117, Center: (662, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 1.0ms preprocess, 49.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 421428, Center: (668, 449)\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 1.2ms preprocess, 55.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 384648, Center: (632, 450)\n",
      "\n",
      "0: 384x640 1 person, 57.0ms\n",
      "Speed: 1.4ms preprocess, 57.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415090, Center: (665, 451)\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 1.5ms preprocess, 50.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413770, Center: (665, 453)\n",
      "\n",
      "0: 384x640 1 person, 52.0ms\n",
      "Speed: 1.0ms preprocess, 52.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402480, Center: (655, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.1ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413770, Center: (666, 454)\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 1.1ms preprocess, 53.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411532, Center: (652, 453)\n",
      "\n",
      "0: 384x640 1 person, 55.3ms\n",
      "Speed: 1.7ms preprocess, 55.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412377, Center: (666, 453)\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 1.1ms preprocess, 52.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 404430, Center: (661, 454)\n",
      "\n",
      "0: 384x640 1 person, 52.7ms\n",
      "Speed: 1.1ms preprocess, 52.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414284, Center: (666, 453)\n",
      "\n",
      "0: 384x640 1 person, 50.8ms\n",
      "Speed: 1.5ms preprocess, 50.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420968, Center: (666, 449)\n",
      "\n",
      "0: 384x640 1 person, 54.9ms\n",
      "Speed: 2.0ms preprocess, 54.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385451, Center: (627, 448)\n",
      "\n",
      "0: 384x640 1 person, 47.5ms\n",
      "Speed: 1.6ms preprocess, 47.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 380120, Center: (623, 450)\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 1.5ms preprocess, 54.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 418253, Center: (665, 450)\n",
      "\n",
      "0: 384x640 1 person, 108.9ms\n",
      "Speed: 1.3ms preprocess, 108.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415151, Center: (664, 450)\n",
      "\n",
      "0: 384x640 1 person, 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415512, Center: (659, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 1.1ms preprocess, 49.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 421015, Center: (665, 449)\n",
      "\n",
      "0: 384x640 1 person, 53.1ms\n",
      "Speed: 1.5ms preprocess, 53.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411532, Center: (660, 451)\n",
      "\n",
      "0: 384x640 1 person, 79.7ms\n",
      "Speed: 1.8ms preprocess, 79.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 432667, Center: (672, 446)\n",
      "\n",
      "0: 384x640 1 person, 51.1ms\n",
      "Speed: 1.5ms preprocess, 51.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416034, Center: (661, 450)\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 1.7ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414739, Center: (657, 448)\n",
      "\n",
      "0: 384x640 1 person, 47.0ms\n",
      "Speed: 1.2ms preprocess, 47.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419688, Center: (660, 448)\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 1.1ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 418080, Center: (662, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 1.5ms preprocess, 44.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422820, Center: (668, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.0ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 425199, Center: (669, 448)\n",
      "\n",
      "0: 384x640 1 person, 52.5ms\n",
      "Speed: 1.6ms preprocess, 52.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415151, Center: (664, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 1.9ms preprocess, 44.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416702, Center: (666, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 1.1ms preprocess, 49.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422584, Center: (667, 448)\n",
      "\n",
      "0: 384x640 1 person, 47.4ms\n",
      "Speed: 1.7ms preprocess, 47.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410736, Center: (662, 452)\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 1.5ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414162, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 2.4ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411939, Center: (666, 453)\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419580, Center: (667, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 1.5ms preprocess, 44.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422760, Center: (670, 449)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.3ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409940, Center: (662, 452)\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 1.5ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382976, Center: (636, 454)\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 1.0ms preprocess, 47.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 376586, Center: (623, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 1.3ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417118, Center: (658, 448)\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 1.3ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 383097, Center: (631, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 1.1ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 406362, Center: (658, 450)\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 2.0ms preprocess, 43.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 405546, Center: (659, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 1.0ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412284, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 1.2ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417842, Center: (665, 449)\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413643, Center: (661, 450)\n",
      "\n",
      "0: 384x640 1 person, 56.0ms\n",
      "Speed: 1.1ms preprocess, 56.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412800, Center: (662, 451)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 1.3ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417444, Center: (667, 451)\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 1.4ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 426586, Center: (667, 447)\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 1.3ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 424725, Center: (667, 447)\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 1.2ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 387624, Center: (629, 449)\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 1.7ms preprocess, 46.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 423630, Center: (666, 448)\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 1.1ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 427397, Center: (668, 445)\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 1.3ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 418122, Center: (662, 448)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 2.0ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415237, Center: (658, 448)\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 1.3ms preprocess, 40.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417321, Center: (659, 448)\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 1.7ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416034, Center: (657, 448)\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414681, Center: (659, 449)\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 1.5ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417040, Center: (660, 450)\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 1.1ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409491, Center: (654, 450)\n",
      "\n",
      "0: 384x640 1 person, 48.1ms\n",
      "Speed: 1.6ms preprocess, 48.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416520, Center: (662, 449)\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 1.9ms preprocess, 49.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410280, Center: (654, 449)\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 1.1ms preprocess, 45.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 392600, Center: (634, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 1.4ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411768, Center: (662, 450)\n",
      "\n",
      "0: 384x640 2 persons, 48.1ms\n",
      "Speed: 1.5ms preprocess, 48.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 405576, Center: (654, 451)\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 1.5ms preprocess, 50.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414634, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 376586, Center: (621, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.6ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 365568, Center: (618, 455)\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 1.7ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409506, Center: (658, 449)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.7ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 399111, Center: (648, 449)\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 1.5ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409981, Center: (662, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 1.2ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400384, Center: (656, 453)\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410220, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 52.3ms\n",
      "Speed: 1.8ms preprocess, 52.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 369566, Center: (621, 452)\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 1.0ms preprocess, 49.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 406756, Center: (663, 453)\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 1.3ms preprocess, 45.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 368334, Center: (620, 453)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 1.5ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 377444, Center: (633, 456)\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 2.1ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410686, Center: (665, 453)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.7ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410970, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 1.5ms preprocess, 42.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 375950, Center: (626, 452)\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 1.7ms preprocess, 45.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413316, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 48.2ms\n",
      "Speed: 1.3ms preprocess, 48.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411485, Center: (663, 452)\n",
      "\n",
      "0: 384x640 1 person, 59.5ms\n",
      "Speed: 1.2ms preprocess, 59.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 381159, Center: (630, 453)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 1.4ms preprocess, 73.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 407365, Center: (660, 450)\n",
      "\n",
      "0: 384x640 1 person, 97.3ms\n",
      "Speed: 1.1ms preprocess, 97.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386428, Center: (631, 451)\n",
      "\n",
      "0: 384x640 1 person, 87.0ms\n",
      "Speed: 1.2ms preprocess, 87.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 401709, Center: (650, 450)\n",
      "\n",
      "0: 384x640 1 person, 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414918, Center: (662, 450)\n",
      "\n",
      "0: 384x640 1 person, 58.9ms\n",
      "Speed: 1.6ms preprocess, 58.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422061, Center: (665, 449)\n",
      "\n",
      "0: 384x640 1 person, 58.9ms\n",
      "Speed: 1.2ms preprocess, 58.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 423864, Center: (664, 449)\n",
      "\n",
      "0: 384x640 1 person, 52.7ms\n",
      "Speed: 1.5ms preprocess, 52.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 380880, Center: (615, 445)\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 1.1ms preprocess, 49.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 374514, Center: (617, 451)\n",
      "\n",
      "0: 384x640 1 person, 54.6ms\n",
      "Speed: 1.7ms preprocess, 54.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 407666, Center: (647, 450)\n",
      "\n",
      "0: 384x640 1 person, 52.2ms\n",
      "Speed: 1.4ms preprocess, 52.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410774, Center: (658, 451)\n",
      "\n",
      "0: 384x640 1 person, 54.1ms\n",
      "Speed: 1.0ms preprocess, 54.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 371520, Center: (620, 452)\n",
      "\n",
      "0: 384x640 1 person, 50.5ms\n",
      "Speed: 1.2ms preprocess, 50.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402976, Center: (657, 452)\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 1.3ms preprocess, 52.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 368740, Center: (619, 451)\n",
      "\n",
      "0: 384x640 1 person, 53.7ms\n",
      "Speed: 1.3ms preprocess, 53.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415151, Center: (667, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.4ms\n",
      "Speed: 1.1ms preprocess, 47.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 403410, Center: (658, 454)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.9ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402462, Center: (656, 453)\n",
      "\n",
      "0: 384x640 1 person, 54.7ms\n",
      "Speed: 1.1ms preprocess, 54.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412515, Center: (664, 452)\n",
      "\n",
      "0: 384x640 1 person, 56.5ms\n",
      "Speed: 1.6ms preprocess, 56.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411768, Center: (663, 453)\n",
      "\n",
      "0: 384x640 1 person, 61.6ms\n",
      "Speed: 2.0ms preprocess, 61.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409887, Center: (663, 454)\n",
      "\n",
      "0: 384x640 1 person, 47.5ms\n",
      "Speed: 1.5ms preprocess, 47.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414348, Center: (665, 453)\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 1.7ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 375132, Center: (623, 452)\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 1.1ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 379260, Center: (627, 451)\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 1.2ms preprocess, 57.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 381546, Center: (630, 451)\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411810, Center: (659, 451)\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 1.3ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413124, Center: (659, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382580, Center: (627, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 1.9ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382200, Center: (621, 448)\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 383760, Center: (626, 450)\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 1.6ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 374796, Center: (617, 449)\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 378129, Center: (617, 449)\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 407422, Center: (653, 449)\n",
      "\n",
      "0: 384x640 1 person, 48.1ms\n",
      "Speed: 1.2ms preprocess, 48.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417877, Center: (660, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.5ms\n",
      "Speed: 1.0ms preprocess, 47.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417560, Center: (660, 451)\n",
      "\n",
      "0: 384x640 1 person, 49.5ms\n",
      "Speed: 1.0ms preprocess, 49.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410292, Center: (647, 451)\n",
      "\n",
      "0: 384x640 1 person, 48.5ms\n",
      "Speed: 1.6ms preprocess, 48.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417040, Center: (662, 451)\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 1.9ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 423150, Center: (662, 447)\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 1.2ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414990, Center: (657, 448)\n",
      "\n",
      "0: 384x640 1 person, 97.8ms\n",
      "Speed: 1.1ms preprocess, 97.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 426343, Center: (663, 446)\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.6ms preprocess, 65.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410400, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 1.8ms preprocess, 79.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415380, Center: (665, 451)\n",
      "\n",
      "0: 384x640 1 person, 59.7ms\n",
      "Speed: 1.7ms preprocess, 59.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410970, Center: (661, 450)\n",
      "\n",
      "0: 384x640 1 person, 65.7ms\n",
      "Speed: 1.3ms preprocess, 65.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 389064, Center: (635, 451)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 1.8ms preprocess, 75.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402187, Center: (646, 447)\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 1.9ms preprocess, 73.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382872, Center: (634, 453)\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 1.2ms preprocess, 51.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 395478, Center: (648, 449)\n",
      "\n",
      "0: 384x640 1 person, 53.1ms\n",
      "Speed: 1.0ms preprocess, 53.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 380730, Center: (630, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 1.2ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 403968, Center: (658, 454)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 2.2ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 377495, Center: (625, 452)\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 397575, Center: (651, 452)\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 1.4ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 383977, Center: (627, 449)\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 1.1ms preprocess, 48.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382313, Center: (624, 448)\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 399641, Center: (650, 450)\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410529, Center: (656, 448)\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 1.2ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417842, Center: (663, 450)\n",
      "\n",
      "0: 384x640 1 person, 46.7ms\n",
      "Speed: 1.5ms preprocess, 46.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416238, Center: (663, 450)\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 1.7ms preprocess, 43.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417150, Center: (666, 451)\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 1.1ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411810, Center: (655, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.4ms\n",
      "Speed: 2.6ms preprocess, 47.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415200, Center: (662, 449)\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 0.9ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 405576, Center: (644, 448)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.0ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 389934, Center: (632, 449)\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 2.2ms preprocess, 44.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400668, Center: (648, 450)\n",
      "\n",
      "0: 384x640 1 person, 48.5ms\n",
      "Speed: 1.0ms preprocess, 48.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385758, Center: (629, 449)\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 2.1ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409228, Center: (651, 447)\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 1.7ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419492, Center: (659, 447)\n",
      "\n",
      "0: 384x640 2 persons, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411601, Center: (652, 448)\n",
      "\n",
      "0: 384x640 2 persons, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408947, Center: (659, 452)\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 1.2ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414739, Center: (658, 448)\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 422010, Center: (666, 450)\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 1.3ms preprocess, 42.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 404790, Center: (656, 452)\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414918, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 2.1ms preprocess, 42.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416472, Center: (665, 452)\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 1.1ms preprocess, 44.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417354, Center: (662, 449)\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 1.5ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386188, Center: (626, 449)\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 1.5ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 383320, Center: (628, 451)\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410800, Center: (656, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 1.1ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409770, Center: (653, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 1.6ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415719, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 2.1ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415954, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.1ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414918, Center: (661, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.2ms\n",
      "Speed: 1.1ms preprocess, 49.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419287, Center: (669, 451)\n",
      "\n",
      "0: 384x640 1 person, 76.2ms\n",
      "Speed: 1.7ms preprocess, 76.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417795, Center: (662, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 1.1ms preprocess, 49.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420800, Center: (662, 447)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 1.9ms preprocess, 72.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417600, Center: (662, 448)\n",
      "\n",
      "0: 384x640 1 person, 111.2ms\n",
      "Speed: 12.8ms preprocess, 111.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 426096, Center: (665, 446)\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 2.2ms preprocess, 62.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419405, Center: (663, 450)\n",
      "\n",
      "0: 384x640 2 persons, 46.9ms\n",
      "Speed: 2.2ms preprocess, 46.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413153, Center: (658, 449)\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 1.5ms preprocess, 48.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419120, Center: (667, 450)\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 421355, Center: (670, 451)\n",
      "\n",
      "0: 384x640 1 person, 47.6ms\n",
      "Speed: 1.2ms preprocess, 47.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412000, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 1.8ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 415312, Center: (666, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 404992, Center: (659, 453)\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412880, Center: (648, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414960, Center: (666, 450)\n",
      "\n",
      "0: 384x640 2 persons, 44.4ms\n",
      "Speed: 1.2ms preprocess, 44.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 372400, Center: (646, 466)\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 1.5ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 398310, Center: (655, 455)\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410333, Center: (667, 454)\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 2.1ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385664, Center: (629, 449)\n",
      "\n",
      "0: 384x640 1 person, 49.6ms\n",
      "Speed: 13.5ms preprocess, 49.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417508, Center: (669, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 1.4ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413545, Center: (667, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 2.0ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 388188, Center: (630, 449)\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 1.5ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 418676, Center: (664, 448)\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 1.2ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416279, Center: (662, 448)\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 1.2ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409600, Center: (664, 454)\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 388784, Center: (636, 451)\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410800, Center: (655, 450)\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 1.0ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 390520, Center: (633, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 1.8ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416000, Center: (663, 449)\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 1.1ms preprocess, 47.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 390750, Center: (632, 449)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413124, Center: (659, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 1.1ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411532, Center: (661, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419120, Center: (665, 450)\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408702, Center: (653, 449)\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 1.0ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386880, Center: (631, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 1.8ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 396516, Center: (639, 450)\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 1.1ms preprocess, 46.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414468, Center: (658, 448)\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408450, Center: (650, 447)\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 1.5ms preprocess, 51.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412605, Center: (661, 450)\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 1.1ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416308, Center: (661, 448)\n",
      "\n",
      "0: 384x640 1 person, 51.6ms\n",
      "Speed: 1.5ms preprocess, 51.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414162, Center: (662, 451)\n",
      "\n",
      "0: 384x640 1 person, 62.1ms\n",
      "Speed: 1.5ms preprocess, 62.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409738, Center: (657, 451)\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 1.3ms preprocess, 83.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416238, Center: (664, 449)\n",
      "\n",
      "0: 384x640 1 person, 103.3ms\n",
      "Speed: 1.2ms preprocess, 103.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402225, Center: (649, 448)\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 1.2ms preprocess, 66.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408630, Center: (658, 452)\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 1.0ms preprocess, 46.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409704, Center: (659, 452)\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 401418, Center: (646, 449)\n",
      "\n",
      "0: 384x640 1 person, 55.8ms\n",
      "Speed: 1.2ms preprocess, 55.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 377332, Center: (619, 445)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 1.4ms preprocess, 70.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 374616, Center: (624, 452)\n",
      "\n",
      "0: 384x640 1 person, 55.8ms\n",
      "Speed: 1.0ms preprocess, 55.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 387103, Center: (631, 449)\n",
      "\n",
      "0: 384x640 1 person, 99.2ms\n",
      "Speed: 1.7ms preprocess, 99.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410400, Center: (666, 453)\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 1.5ms preprocess, 52.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 412880, Center: (664, 449)\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 1.2ms preprocess, 51.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 406528, Center: (662, 453)\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 1.1ms preprocess, 52.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 401920, Center: (658, 454)\n",
      "\n",
      "0: 384x640 1 person, 64.9ms\n",
      "Speed: 1.7ms preprocess, 64.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410550, Center: (667, 454)\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 1.6ms preprocess, 66.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385392, Center: (632, 451)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 1.2ms preprocess, 69.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385617, Center: (630, 450)\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 1.7ms preprocess, 61.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413674, Center: (662, 449)\n",
      "\n",
      "0: 384x640 1 person, 66.3ms\n",
      "Speed: 1.5ms preprocess, 66.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 416990, Center: (665, 451)\n",
      "\n",
      "0: 384x640 1 person, 48.2ms\n",
      "Speed: 1.1ms preprocess, 48.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414348, Center: (663, 452)\n",
      "\n",
      "0: 384x640 1 person, 49.2ms\n",
      "Speed: 1.1ms preprocess, 49.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 419871, Center: (667, 450)\n",
      "\n",
      "0: 384x640 1 person, 52.8ms\n",
      "Speed: 2.0ms preprocess, 52.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414864, Center: (663, 451)\n",
      "\n",
      "0: 384x640 1 person, 64.0ms\n",
      "Speed: 1.1ms preprocess, 64.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 420098, Center: (665, 450)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 1.4ms preprocess, 70.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 424840, Center: (670, 450)\n",
      "\n",
      "0: 384x640 1 person, 66.9ms\n",
      "Speed: 1.2ms preprocess, 66.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 423342, Center: (666, 450)\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 1.2ms preprocess, 46.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 424542, Center: (670, 451)\n",
      "\n",
      "0: 384x640 1 person, 52.8ms\n",
      "Speed: 1.6ms preprocess, 52.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 423504, Center: (671, 450)\n",
      "\n",
      "0: 384x640 1 person, 53.9ms\n",
      "Speed: 1.7ms preprocess, 53.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 384420, Center: (630, 452)\n",
      "\n",
      "0: 384x640 1 person, 50.8ms\n",
      "Speed: 1.2ms preprocess, 50.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386946, Center: (631, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 1.0ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 414117, Center: (663, 450)\n",
      "\n",
      "0: 384x640 1 person, 47.7ms\n",
      "Speed: 1.2ms preprocess, 47.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 417508, Center: (666, 450)\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 1.0ms preprocess, 49.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400932, Center: (651, 449)\n",
      "\n",
      "0: 384x640 1 person, 50.7ms\n",
      "Speed: 1.5ms preprocess, 50.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 409425, Center: (662, 452)\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 1.1ms preprocess, 63.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 367195, Center: (617, 451)\n",
      "\n",
      "0: 384x640 1 person, 53.1ms\n",
      "Speed: 1.0ms preprocess, 53.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 402212, Center: (654, 449)\n",
      "\n",
      "0: 384x640 1 person, 56.5ms\n",
      "Speed: 0.9ms preprocess, 56.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 382580, Center: (630, 451)\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 1.6ms preprocess, 50.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 407160, Center: (652, 450)\n",
      "\n",
      "0: 384x640 1 person, 48.6ms\n",
      "Speed: 1.2ms preprocess, 48.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385320, Center: (643, 456)\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 1.6ms preprocess, 49.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 397312, Center: (652, 453)\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 1.2ms preprocess, 46.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400624, Center: (655, 454)\n",
      "\n",
      "0: 384x640 1 person, 47.6ms\n",
      "Speed: 1.6ms preprocess, 47.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408861, Center: (659, 452)\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 1.0ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400155, Center: (649, 453)\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 1.5ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411567, Center: (655, 451)\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 1.0ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 395523, Center: (643, 453)\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411200, Center: (661, 453)\n",
      "\n",
      "0: 384x640 1 person, 50.9ms\n",
      "Speed: 1.1ms preprocess, 50.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 392616, Center: (651, 457)\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 1.0ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 399627, Center: (649, 453)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 1.0ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 397062, Center: (650, 452)\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 1.3ms preprocess, 45.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 377495, Center: (625, 452)\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408156, Center: (653, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 1.2ms preprocess, 45.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 401434, Center: (653, 451)\n",
      "\n",
      "0: 384x640 2 persons, 49.8ms\n",
      "Speed: 2.3ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 388548, Center: (636, 452)\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400668, Center: (645, 450)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 1.4ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 413030, Center: (664, 451)\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 1.5ms preprocess, 45.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411485, Center: (661, 451)\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 411768, Center: (664, 452)\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 410455, Center: (663, 453)\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 1.5ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 398601, Center: (654, 452)\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385315, Center: (646, 455)\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 1.1ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 395576, Center: (658, 459)\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 1.5ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 385320, Center: (644, 456)\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 391421, Center: (648, 455)\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 371480, Center: (635, 459)\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 1.3ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 388620, Center: (644, 455)\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 390126, Center: (648, 456)\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 384795, Center: (647, 458)\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 1.1ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 384954, Center: (651, 461)\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 1.4ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386000, Center: (651, 462)\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 2.1ms preprocess, 42.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 395186, Center: (656, 457)\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.3ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 390780, Center: (655, 459)\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 1.4ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 386500, Center: (650, 461)\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 1.4ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 389050, Center: (652, 460)\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 4.7ms preprocess, 62.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 389592, Center: (652, 458)\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 1.2ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 391104, Center: (652, 458)\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 1.0ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 394128, Center: (656, 458)\n",
      "\n",
      "0: 384x640 1 person, 48.5ms\n",
      "Speed: 1.6ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 396704, Center: (655, 456)\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 1.6ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 401544, Center: (659, 456)\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 400891, Center: (660, 458)\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 391668, Center: (650, 455)\n",
      "\n",
      "0: 384x640 1 person, 45.2ms\n",
      "Speed: 1.2ms preprocess, 45.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 406380, Center: (653, 448)\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 407088, Center: (661, 453)\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 1.5ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 408630, Center: (660, 452)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.1ms preprocess, 75.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Area: 383160, Center: (630, 452)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m model(frame)  \u001b[38;5;66;03m# Run YOLO detection\u001b[39;00m\n\u001b[1;32m     16\u001b[0m myPersonList \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m myPersonListArea \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:558\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 558\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:109\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:127\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:148\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    149\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:348\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is more accurate\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "# Smallest YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Inference on the frame \n",
    "    results = model(frame) \n",
    "\n",
    "    myPersonList = []\n",
    "    myPersonListArea = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get class ID\n",
    "            cls = int(box.cls[0]) \n",
    "            # Confidence score \n",
    "            conf = box.conf[0].item()  \n",
    "            # Class 0 = \"person\", confidence > 80%\n",
    "            if cls == 0 and conf > 0.8:  \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                # Center of bounding box\n",
    "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2  \n",
    "                # Calculate area\n",
    "                area = (x2 - x1) * (y2 - y1)  \n",
    "                \n",
    "                # Depending on the size of the bounding box the drone will move\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) \n",
    "                # This is what the drone will follow\n",
    "                # Draw center point\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED) \n",
    "\n",
    "                myPersonList.append((cx, cy))\n",
    "                myPersonListArea.append(area)\n",
    "\n",
    "    # Select the largest detected person\n",
    "    if myPersonList:\n",
    "        i = myPersonListArea.index(max(myPersonListArea))\n",
    "        print(f'Area: {myPersonListArea[i]}, Center: {myPersonList[i]}')\n",
    "\n",
    "    cv2.imshow(\"Person Detection (YOLOv8)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
